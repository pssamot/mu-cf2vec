{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_vae = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just choose the name of the dataset directory\n",
    "DATA_DIR = '/Users/tomas/Documents/FEUP/Tese/data/ml-20m/processed_70_10_20'\n",
    "if is_vae:\n",
    "    PARSE_DATA_DIR = os.path.join(DATA_DIR, 'embeddings/vae')\n",
    "else:\n",
    "    PARSE_DATA_DIR = os.path.join(DATA_DIR, 'embeddings/cdae')\n",
    "    file = '200_fac_metadataset_k_20.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'metadataset_k_20.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>first_place</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7648</td>\n",
       "      <td>-0.431028</td>\n",
       "      <td>0.273519</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>-0.021366</td>\n",
       "      <td>0.411798</td>\n",
       "      <td>0.445795</td>\n",
       "      <td>1.206461</td>\n",
       "      <td>0.437548</td>\n",
       "      <td>-0.670044</td>\n",
       "      <td>...</td>\n",
       "      <td>1.261934</td>\n",
       "      <td>0.238986</td>\n",
       "      <td>0.150137</td>\n",
       "      <td>-0.024725</td>\n",
       "      <td>-0.349221</td>\n",
       "      <td>3.509393</td>\n",
       "      <td>-0.146421</td>\n",
       "      <td>1.508804</td>\n",
       "      <td>-0.859744</td>\n",
       "      <td>als_ndcg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10208</td>\n",
       "      <td>0.351687</td>\n",
       "      <td>-0.626017</td>\n",
       "      <td>0.034143</td>\n",
       "      <td>-0.080265</td>\n",
       "      <td>0.101129</td>\n",
       "      <td>-0.229498</td>\n",
       "      <td>0.261393</td>\n",
       "      <td>-1.189405</td>\n",
       "      <td>-0.451783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.964082</td>\n",
       "      <td>0.825656</td>\n",
       "      <td>0.064932</td>\n",
       "      <td>0.042978</td>\n",
       "      <td>-1.043557</td>\n",
       "      <td>0.562128</td>\n",
       "      <td>0.234560</td>\n",
       "      <td>0.813003</td>\n",
       "      <td>0.511105</td>\n",
       "      <td>zeroes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13315</td>\n",
       "      <td>-0.706174</td>\n",
       "      <td>-0.031290</td>\n",
       "      <td>-0.005091</td>\n",
       "      <td>-0.097111</td>\n",
       "      <td>0.111871</td>\n",
       "      <td>0.484369</td>\n",
       "      <td>0.737223</td>\n",
       "      <td>-1.132764</td>\n",
       "      <td>1.119018</td>\n",
       "      <td>...</td>\n",
       "      <td>1.040254</td>\n",
       "      <td>-1.375516</td>\n",
       "      <td>-0.064951</td>\n",
       "      <td>0.030435</td>\n",
       "      <td>0.757224</td>\n",
       "      <td>0.444452</td>\n",
       "      <td>0.753969</td>\n",
       "      <td>-0.582505</td>\n",
       "      <td>0.761838</td>\n",
       "      <td>bpr_ndcg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16144</td>\n",
       "      <td>0.672244</td>\n",
       "      <td>-0.958536</td>\n",
       "      <td>-0.005133</td>\n",
       "      <td>-0.093083</td>\n",
       "      <td>0.118219</td>\n",
       "      <td>-0.325690</td>\n",
       "      <td>1.434977</td>\n",
       "      <td>0.006304</td>\n",
       "      <td>-0.780396</td>\n",
       "      <td>...</td>\n",
       "      <td>1.159395</td>\n",
       "      <td>-0.746610</td>\n",
       "      <td>0.042197</td>\n",
       "      <td>0.038217</td>\n",
       "      <td>-0.813577</td>\n",
       "      <td>1.753534</td>\n",
       "      <td>-0.063353</td>\n",
       "      <td>-0.829087</td>\n",
       "      <td>-1.122440</td>\n",
       "      <td>most_popular_ndcg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18064</td>\n",
       "      <td>-0.813108</td>\n",
       "      <td>0.897909</td>\n",
       "      <td>-0.105261</td>\n",
       "      <td>0.080410</td>\n",
       "      <td>0.099298</td>\n",
       "      <td>-1.109625</td>\n",
       "      <td>2.775797</td>\n",
       "      <td>0.139941</td>\n",
       "      <td>-0.745728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.584199</td>\n",
       "      <td>0.619601</td>\n",
       "      <td>0.027197</td>\n",
       "      <td>0.051431</td>\n",
       "      <td>-1.668557</td>\n",
       "      <td>1.609979</td>\n",
       "      <td>-1.117613</td>\n",
       "      <td>1.222671</td>\n",
       "      <td>-2.145877</td>\n",
       "      <td>lmf_ndcg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   original_id         1         2         3         4         5         6  \\\n",
       "0         7648 -0.431028  0.273519  0.000467 -0.021366  0.411798  0.445795   \n",
       "1        10208  0.351687 -0.626017  0.034143 -0.080265  0.101129 -0.229498   \n",
       "2        13315 -0.706174 -0.031290 -0.005091 -0.097111  0.111871  0.484369   \n",
       "3        16144  0.672244 -0.958536 -0.005133 -0.093083  0.118219 -0.325690   \n",
       "4        18064 -0.813108  0.897909 -0.105261  0.080410  0.099298 -1.109625   \n",
       "\n",
       "          7         8         9  ...       192       193       194       195  \\\n",
       "0  1.206461  0.437548 -0.670044  ...  1.261934  0.238986  0.150137 -0.024725   \n",
       "1  0.261393 -1.189405 -0.451783  ...  0.964082  0.825656  0.064932  0.042978   \n",
       "2  0.737223 -1.132764  1.119018  ...  1.040254 -1.375516 -0.064951  0.030435   \n",
       "3  1.434977  0.006304 -0.780396  ...  1.159395 -0.746610  0.042197  0.038217   \n",
       "4  2.775797  0.139941 -0.745728  ...  0.584199  0.619601  0.027197  0.051431   \n",
       "\n",
       "        196       197       198       199       200        first_place  \n",
       "0 -0.349221  3.509393 -0.146421  1.508804 -0.859744           als_ndcg  \n",
       "1 -1.043557  0.562128  0.234560  0.813003  0.511105             zeroes  \n",
       "2  0.757224  0.444452  0.753969 -0.582505  0.761838           bpr_ndcg  \n",
       "3 -0.813577  1.753534 -0.063353 -0.829087 -1.122440  most_popular_ndcg  \n",
       "4 -1.668557  1.609979 -1.117613  1.222671 -2.145877           lmf_ndcg  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the data using pandas\n",
    "metadataset = pd.read_csv(os.path.join(PARSE_DATA_DIR, file ))\n",
    "metadataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count %  34215\n",
      "als_ndcg  %  0.2636851961743875\n",
      "count %  29088\n",
      "most_popular_ndcg  %  0.22417287699314872\n",
      "count %  25925\n",
      "bpr_ndcg  %  0.19979654276840556\n",
      "count %  21277\n",
      "zeroes  %  0.16397573926647502\n",
      "count %  19252\n",
      "lmf_ndcg  %  0.14836964479758316\n"
     ]
    }
   ],
   "source": [
    "total = metadataset.shape[0]\n",
    "names = ['als_ndcg','most_popular_ndcg','bpr_ndcg','zeroes','lmf_ndcg']\n",
    "for name, count in zip(names,metadataset.first_place.value_counts()):\n",
    "    print(\"count % \",count)\n",
    "    print(name,\" % \",count/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#als:0\n",
    "#bpr:1\n",
    "#lmf:2\n",
    "#most_pop_3\n",
    "#zeros:4\n",
    "target_pre = metadataset['first_place'].values \n",
    "label_encoder = LabelEncoder()\n",
    "target = label_encoder.fit_transform(target_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if normalize:\n",
    "  #---- SET INPUTS -----\n",
    "  scaler = StandardScaler()\n",
    "  #Compute the mean and std to be used for later scaling.\n",
    "  scaler.fit(metadataset.drop(columns=['first_place','original_id']))\n",
    "  # Perform standardization by centering and scaling\n",
    "  inputs_transform = scaler.transform(metadataset.drop(columns=['first_place','original_id']))\n",
    "  inputs = pd.DataFrame(inputs_transform)\n",
    "  inputs.head()\n",
    "else:\n",
    "  inputs = metadataset.drop(columns=['first_place','original_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.762653</td>\n",
       "      <td>0.502876</td>\n",
       "      <td>-0.440999</td>\n",
       "      <td>0.578740</td>\n",
       "      <td>6.487320</td>\n",
       "      <td>0.846224</td>\n",
       "      <td>1.059278</td>\n",
       "      <td>0.777620</td>\n",
       "      <td>-1.362870</td>\n",
       "      <td>0.132310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.737741</td>\n",
       "      <td>2.757368</td>\n",
       "      <td>0.472769</td>\n",
       "      <td>2.932462</td>\n",
       "      <td>-0.474176</td>\n",
       "      <td>-0.526718</td>\n",
       "      <td>3.702920</td>\n",
       "      <td>-0.302123</td>\n",
       "      <td>1.995282</td>\n",
       "      <td>-1.627410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.780034</td>\n",
       "      <td>-1.231799</td>\n",
       "      <td>0.411887</td>\n",
       "      <td>-0.783547</td>\n",
       "      <td>1.087678</td>\n",
       "      <td>-0.557226</td>\n",
       "      <td>0.164737</td>\n",
       "      <td>-2.081762</td>\n",
       "      <td>-0.935257</td>\n",
       "      <td>0.612950</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.196858</td>\n",
       "      <td>2.117003</td>\n",
       "      <td>1.549487</td>\n",
       "      <td>1.140396</td>\n",
       "      <td>1.089726</td>\n",
       "      <td>-1.668867</td>\n",
       "      <td>0.355279</td>\n",
       "      <td>0.447397</td>\n",
       "      <td>1.053020</td>\n",
       "      <td>0.980292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.304951</td>\n",
       "      <td>-0.084922</td>\n",
       "      <td>-0.581762</td>\n",
       "      <td>-1.173176</td>\n",
       "      <td>1.274384</td>\n",
       "      <td>0.926392</td>\n",
       "      <td>0.615127</td>\n",
       "      <td>-1.982215</td>\n",
       "      <td>2.142214</td>\n",
       "      <td>-0.955726</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042859</td>\n",
       "      <td>2.280769</td>\n",
       "      <td>-2.490335</td>\n",
       "      <td>-1.591350</td>\n",
       "      <td>0.799997</td>\n",
       "      <td>1.293329</td>\n",
       "      <td>0.221618</td>\n",
       "      <td>1.469252</td>\n",
       "      <td>-0.836794</td>\n",
       "      <td>1.457249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.411833</td>\n",
       "      <td>-1.873034</td>\n",
       "      <td>-0.582824</td>\n",
       "      <td>-1.080004</td>\n",
       "      <td>1.384711</td>\n",
       "      <td>-0.757140</td>\n",
       "      <td>1.275576</td>\n",
       "      <td>0.019706</td>\n",
       "      <td>-1.579068</td>\n",
       "      <td>-1.203121</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283741</td>\n",
       "      <td>2.536914</td>\n",
       "      <td>-1.336100</td>\n",
       "      <td>0.662212</td>\n",
       "      <td>0.979756</td>\n",
       "      <td>-1.290560</td>\n",
       "      <td>1.708534</td>\n",
       "      <td>-0.138699</td>\n",
       "      <td>-1.170718</td>\n",
       "      <td>-2.127123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.515711</td>\n",
       "      <td>1.706955</td>\n",
       "      <td>-3.118682</td>\n",
       "      <td>2.932744</td>\n",
       "      <td>1.055853</td>\n",
       "      <td>-2.386381</td>\n",
       "      <td>2.544711</td>\n",
       "      <td>0.254573</td>\n",
       "      <td>-1.511147</td>\n",
       "      <td>-1.952087</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.104837</td>\n",
       "      <td>1.300275</td>\n",
       "      <td>1.171314</td>\n",
       "      <td>0.346739</td>\n",
       "      <td>1.284994</td>\n",
       "      <td>-2.696961</td>\n",
       "      <td>1.545478</td>\n",
       "      <td>-2.212789</td>\n",
       "      <td>1.607798</td>\n",
       "      <td>-4.073960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.762653  0.502876 -0.440999  0.578740  6.487320  0.846224  1.059278   \n",
       "1  0.780034 -1.231799  0.411887 -0.783547  1.087678 -0.557226  0.164737   \n",
       "2 -1.304951 -0.084922 -0.581762 -1.173176  1.274384  0.926392  0.615127   \n",
       "3  1.411833 -1.873034 -0.582824 -1.080004  1.384711 -0.757140  1.275576   \n",
       "4 -1.515711  1.706955 -3.118682  2.932744  1.055853 -2.386381  2.544711   \n",
       "\n",
       "        7         8         9    ...       190       191       192       193  \\\n",
       "0  0.777620 -1.362870  0.132310  ...  0.737741  2.757368  0.472769  2.932462   \n",
       "1 -2.081762 -0.935257  0.612950  ... -1.196858  2.117003  1.549487  1.140396   \n",
       "2 -1.982215  2.142214 -0.955726  ... -0.042859  2.280769 -2.490335 -1.591350   \n",
       "3  0.019706 -1.579068 -1.203121  ... -2.283741  2.536914 -1.336100  0.662212   \n",
       "4  0.254573 -1.511147 -1.952087  ... -1.104837  1.300275  1.171314  0.346739   \n",
       "\n",
       "        194       195       196       197       198       199  \n",
       "0 -0.474176 -0.526718  3.702920 -0.302123  1.995282 -1.627410  \n",
       "1  1.089726 -1.668867  0.355279  0.447397  1.053020  0.980292  \n",
       "2  0.799997  1.293329  0.221618  1.469252 -0.836794  1.457249  \n",
       "3  0.979756 -1.290560  1.708534 -0.138699 -1.170718 -2.127123  \n",
       "4  1.284994 -2.696961  1.545478 -2.212789  1.607798 -4.073960  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits()\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'activation': 'relu',\n",
    "    'alpha': 0.05,\n",
    "    'hidden_layer_sizes': (50, 50, 50),\n",
    "    'learning_rate': 'constant',\n",
    "    'solver': 'sgd'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  1\n",
      "fit\n",
      "Iteration 1, loss = 1.63828318\n",
      "Iteration 2, loss = 1.60920346\n",
      "Iteration 3, loss = 1.59945346\n",
      "Iteration 4, loss = 1.59269732\n",
      "Iteration 5, loss = 1.58698897\n",
      "Iteration 6, loss = 1.58234605\n",
      "Iteration 7, loss = 1.57821292\n",
      "Iteration 8, loss = 1.57464012\n",
      "Iteration 9, loss = 1.57145871\n",
      "Iteration 10, loss = 1.56851788\n",
      "Iteration 11, loss = 1.56588084\n",
      "Iteration 12, loss = 1.56337162\n",
      "Iteration 13, loss = 1.56118501\n",
      "Iteration 14, loss = 1.55903726\n",
      "Iteration 15, loss = 1.55702379\n",
      "Iteration 16, loss = 1.55508423\n",
      "Iteration 17, loss = 1.55327655\n",
      "Iteration 18, loss = 1.55152049\n",
      "Iteration 19, loss = 1.54976155\n",
      "Iteration 20, loss = 1.54818858\n",
      "Iteration 21, loss = 1.54666260\n",
      "Iteration 22, loss = 1.54519199\n",
      "Iteration 23, loss = 1.54368514\n",
      "Iteration 24, loss = 1.54233001\n",
      "Iteration 25, loss = 1.54096628\n",
      "Iteration 26, loss = 1.53964596\n",
      "Iteration 27, loss = 1.53846388\n",
      "Iteration 28, loss = 1.53717346\n",
      "Iteration 29, loss = 1.53597488\n",
      "Iteration 30, loss = 1.53481334\n",
      "Iteration 31, loss = 1.53377662\n",
      "Iteration 32, loss = 1.53262025\n",
      "Iteration 33, loss = 1.53154933\n",
      "Iteration 34, loss = 1.53057075\n",
      "Iteration 35, loss = 1.52954767\n",
      "Iteration 36, loss = 1.52848340\n",
      "Iteration 37, loss = 1.52757713\n",
      "Iteration 38, loss = 1.52659825\n",
      "Iteration 39, loss = 1.52563285\n",
      "Iteration 40, loss = 1.52468181\n",
      "Iteration 41, loss = 1.52379275\n",
      "Iteration 42, loss = 1.52291226\n",
      "Iteration 43, loss = 1.52202905\n",
      "Iteration 44, loss = 1.52117704\n",
      "Iteration 45, loss = 1.52032808\n",
      "Iteration 46, loss = 1.51949879\n",
      "Iteration 47, loss = 1.51869546\n",
      "Iteration 48, loss = 1.51778581\n",
      "Iteration 49, loss = 1.51701587\n",
      "Iteration 50, loss = 1.51622819\n",
      "Iteration 51, loss = 1.51541937\n",
      "Iteration 52, loss = 1.51466772\n",
      "Iteration 53, loss = 1.51386814\n",
      "Iteration 54, loss = 1.51312669\n",
      "Iteration 55, loss = 1.51231755\n",
      "Iteration 56, loss = 1.51145406\n",
      "Iteration 57, loss = 1.51084014\n",
      "Iteration 58, loss = 1.51001951\n",
      "Iteration 59, loss = 1.50923974\n",
      "Iteration 60, loss = 1.50852679\n",
      "Iteration 61, loss = 1.50774409\n",
      "Iteration 62, loss = 1.50705614\n",
      "Iteration 63, loss = 1.50636120\n",
      "Iteration 64, loss = 1.50566943\n",
      "Iteration 65, loss = 1.50485043\n",
      "Iteration 66, loss = 1.50422924\n",
      "Iteration 67, loss = 1.50362185\n",
      "Iteration 68, loss = 1.50281471\n",
      "Iteration 69, loss = 1.50219129\n",
      "Iteration 70, loss = 1.50152079\n",
      "Iteration 71, loss = 1.50076297\n",
      "Iteration 72, loss = 1.50010397\n",
      "Iteration 73, loss = 1.49943432\n",
      "Iteration 74, loss = 1.49884998\n",
      "Iteration 75, loss = 1.49814546\n",
      "Iteration 76, loss = 1.49749406\n",
      "Iteration 77, loss = 1.49684730\n",
      "Iteration 78, loss = 1.49618098\n",
      "Iteration 79, loss = 1.49545128\n",
      "Iteration 80, loss = 1.49487618\n",
      "Iteration 81, loss = 1.49427049\n",
      "Iteration 82, loss = 1.49357293\n",
      "Iteration 83, loss = 1.49297909\n",
      "Iteration 84, loss = 1.49237296\n",
      "Iteration 85, loss = 1.49169803\n",
      "Iteration 86, loss = 1.49105018\n",
      "Iteration 87, loss = 1.49043738\n",
      "Iteration 88, loss = 1.48980180\n",
      "Iteration 89, loss = 1.48915868\n",
      "Iteration 90, loss = 1.48856604\n",
      "Iteration 91, loss = 1.48784972\n",
      "Iteration 92, loss = 1.48738995\n",
      "Iteration 93, loss = 1.48674255\n",
      "Iteration 94, loss = 1.48615456\n",
      "Iteration 95, loss = 1.48559944\n",
      "Iteration 96, loss = 1.48500067\n",
      "Iteration 97, loss = 1.48436939\n",
      "Iteration 98, loss = 1.48372802\n",
      "Iteration 99, loss = 1.48313643\n",
      "Iteration 100, loss = 1.48259086\n",
      "Iteration 101, loss = 1.48208994\n",
      "Iteration 102, loss = 1.48147417\n",
      "Iteration 103, loss = 1.48089036\n",
      "Iteration 104, loss = 1.48035610\n",
      "Iteration 105, loss = 1.47976968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:568: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting base_level_eval\n",
      "end:  1\n",
      "iteration:  2\n",
      "fit\n",
      "Iteration 1, loss = 1.63845910\n",
      "Iteration 2, loss = 1.60929621\n",
      "Iteration 3, loss = 1.59936426\n",
      "Iteration 4, loss = 1.59230906\n",
      "Iteration 5, loss = 1.58643718\n",
      "Iteration 6, loss = 1.58140231\n",
      "Iteration 7, loss = 1.57708393\n",
      "Iteration 8, loss = 1.57333025\n",
      "Iteration 9, loss = 1.56998688\n",
      "Iteration 10, loss = 1.56693795\n",
      "Iteration 11, loss = 1.56423254\n",
      "Iteration 12, loss = 1.56165496\n",
      "Iteration 13, loss = 1.55937162\n",
      "Iteration 14, loss = 1.55720850\n",
      "Iteration 15, loss = 1.55515127\n",
      "Iteration 16, loss = 1.55321293\n",
      "Iteration 17, loss = 1.55131962\n",
      "Iteration 18, loss = 1.54960670\n",
      "Iteration 19, loss = 1.54788987\n",
      "Iteration 20, loss = 1.54629600\n",
      "Iteration 21, loss = 1.54473743\n",
      "Iteration 22, loss = 1.54328159\n",
      "Iteration 23, loss = 1.54182392\n",
      "Iteration 24, loss = 1.54054238\n",
      "Iteration 25, loss = 1.53915718\n",
      "Iteration 26, loss = 1.53786583\n",
      "Iteration 27, loss = 1.53661909\n",
      "Iteration 28, loss = 1.53538977\n",
      "Iteration 29, loss = 1.53419154\n",
      "Iteration 30, loss = 1.53313548\n",
      "Iteration 31, loss = 1.53205815\n",
      "Iteration 32, loss = 1.53088977\n",
      "Iteration 33, loss = 1.52986775\n",
      "Iteration 34, loss = 1.52894722\n",
      "Iteration 35, loss = 1.52794536\n",
      "Iteration 36, loss = 1.52692668\n",
      "Iteration 37, loss = 1.52602744\n",
      "Iteration 38, loss = 1.52510288\n",
      "Iteration 39, loss = 1.52415078\n",
      "Iteration 40, loss = 1.52326687\n",
      "Iteration 41, loss = 1.52252961\n",
      "Iteration 42, loss = 1.52156590\n",
      "Iteration 43, loss = 1.52086065\n",
      "Iteration 44, loss = 1.51991234\n",
      "Iteration 45, loss = 1.51910469\n",
      "Iteration 46, loss = 1.51828341\n",
      "Iteration 47, loss = 1.51750532\n",
      "Iteration 48, loss = 1.51672620\n",
      "Iteration 49, loss = 1.51591346\n",
      "Iteration 50, loss = 1.51519858\n",
      "Iteration 51, loss = 1.51437883\n",
      "Iteration 52, loss = 1.51370040\n",
      "Iteration 53, loss = 1.51293617\n",
      "Iteration 54, loss = 1.51220406\n",
      "Iteration 55, loss = 1.51148558\n",
      "Iteration 56, loss = 1.51063383\n",
      "Iteration 57, loss = 1.51012341\n",
      "Iteration 58, loss = 1.50928416\n",
      "Iteration 59, loss = 1.50858646\n",
      "Iteration 60, loss = 1.50793802\n",
      "Iteration 61, loss = 1.50720891\n",
      "Iteration 62, loss = 1.50649173\n"
     ]
    }
   ],
   "source": [
    "i = 1 \n",
    "reports = []\n",
    "base_impact_with_zeroes = []\n",
    "base_impact_without_zeroes_most = []\n",
    "base_impact_without_zeroes_best = []\n",
    "matrix = []\n",
    "for train_index, test_index in kf.split(inputs):\n",
    "    print('iteration: ', i)\n",
    "    #get data fold\n",
    "    X_train, X_test = inputs.iloc[train_index], inputs.iloc[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    \n",
    "    #start model \n",
    "    print('fit')\n",
    "    clf = MLPClassifier(random_state=0, \n",
    "                        max_iter=300,\n",
    "                       activation=params['activation'],\n",
    "                       alpha=params['alpha'],\n",
    "                       hidden_layer_sizes=params['hidden_layer_sizes'],\n",
    "                       learning_rate=params['learning_rate'],\n",
    "                       solver=params['solver'],\n",
    "                       verbose=5)\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    report = classification_report(y_test, \n",
    "                                   y_pred, \n",
    "                                   target_names=np.unique(metadataset['first_place'].values),\n",
    "                                  output_dict=True)\n",
    "    \n",
    "    bl_zeroes, bl_no_zeroes_most, bl_no_zeroes_best = base_level_eval(metadataset.iloc[test_index]['original_id'].values,\n",
    "             list(label_encoder.inverse_transform(y_pred)))\n",
    "\n",
    "    base_impact_with_zeroes.append(bl_zeroes)\n",
    "    base_impact_without_zeroes_most.append(bl_no_zeroes_most)\n",
    "    base_impact_without_zeroes_best.append(bl_no_zeroes_best)\n",
    "\n",
    "    confusion = confusion_matrix(y_test,y_pred)\n",
    "    matrix.append(confusion)\n",
    "    np.set_printoptions(suppress=True)    \n",
    "    reports.append(report)\n",
    "    \n",
    "    print('end: ', i)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_reports = report_average(reports)\n",
    "print_report(avg_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(avg_reports):\n",
    "    from prettytable import PrettyTable\n",
    "    x = PrettyTable()\n",
    "\n",
    "    x.field_names = [\"Algorithm\", \"Precision\", \"Recall\", \"F1\"]\n",
    "\n",
    "    for label in avg_reports.keys():\n",
    "        if label in 'accuracy':\n",
    "            x.add_row(['---','---','---','---'])\n",
    "            continue\n",
    "        x.add_row([label, \n",
    "                   avg_reports[label]['precision'], \n",
    "                   avg_reports[label]['recall'], \n",
    "                   avg_reports[label]['f1-score']])\n",
    "\n",
    "\n",
    "    print(x)\n",
    "    print('Accuracy: ', avg_reports['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion(values, classes):\n",
    "    from prettytable import PrettyTable\n",
    "    x = PrettyTable()\n",
    "    print(classes)\n",
    "\n",
    "    names = []\n",
    "    names.append('algorithm')\n",
    "    names = names + list(classes.values())\n",
    "\n",
    "    x.field_names = names\n",
    "\n",
    "    i = 0\n",
    "    for row in values:\n",
    "        #row = np.array(row)\n",
    "        r = []\n",
    "        r.append(classes[i])\n",
    "        row = r + list(row)\n",
    "        #r.append(classes[i])\n",
    "        #r = r + row\n",
    "        #row = np.insert(row,0,'als')\n",
    "        x.add_row(row)\n",
    "        #r  = np.concatenate(csses[i],row[])\n",
    "        i +=1\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_average(reports):\n",
    "    mean_dict = dict()\n",
    "    for label in reports[0].keys():\n",
    "        dictionary = dict()\n",
    "\n",
    "        if label in 'accuracy':\n",
    "            mean_dict[label] = sum(d[label] for d in reports) / len(reports)\n",
    "            continue\n",
    "\n",
    "        for key in reports[0][label].keys():\n",
    "            dictionary[key] = sum(d[label][key] for d in reports) / len(reports)\n",
    "        mean_dict[label] = dictionary\n",
    "\n",
    "    return mean_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_level_eval(users, predictions):\n",
    "    \"\"\"Uses the predctions to return the average of the ndcg impact at base level.\n",
    "    Args:\n",
    "        users: list of users ids\n",
    "        predictions:predictions for users. PREDS HAVE TO be the same index ahas the users list\n",
    "    Returns:\n",
    "        average of ndcg\n",
    "    \"\"\"\n",
    "    print('starting base_level_eval')\n",
    "    results_algo = pd.read_csv(os.path.join(DATA_DIR, 'results_metadataset.csv'))\n",
    "    base_impact = []\n",
    "    base_impact_zeroes_most = []\n",
    "    base_impact_zeroes_best = []\n",
    "    for user_uid, pred in zip(users, predictions):\n",
    "\n",
    "\n",
    "        val = results_algo.loc[ results_algo['original_id'] == user_uid, pred ]\n",
    "        if pred == 'zeroes':\n",
    "            val_zeroes = results_algo.loc[ results_algo['original_id'] == user_uid, 'most_popular_ndcg']\n",
    "            best = results_algo.loc[ results_algo['original_id'] == user_uid]\n",
    "\n",
    "            base_impact.append(val.values[0])\n",
    "            base_impact_zeroes_most.append(val_zeroes.values[0])\n",
    "            base_impact_zeroes_best.append(best.drop('original_id', 1).max(axis=1).values[0])\n",
    "        else:\n",
    "            base_impact.append(val.values[0])\n",
    "            base_impact_zeroes_most.append(val.values[0])\n",
    "            base_impact_zeroes_best.append(val.values[0])\n",
    "\n",
    "\n",
    "        if len(val.values) > 1:\n",
    "            raise Exception(\"More than one case\")\n",
    "\n",
    "    return np.mean(base_impact), np.mean(base_impact_zeroes_most), np.mean(base_impact_zeroes_best)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grid_params = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed: 19.7min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 60.2min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 134.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
       "                                     batch_size='auto', beta_1=0.9,\n",
       "                                     beta_2=0.999, early_stopping=False,\n",
       "                                     epsilon=1e-08, hidden_layer_sizes=(100,),\n",
       "                                     learning_rate='constant',\n",
       "                                     learning_rate_init=0.001, max_iter=300,\n",
       "                                     momentum=0.9, n_iter_no_change=10,\n",
       "                                     nesterovs_momentum=True, power_t=0.5,\n",
       "                                     random_sta...\n",
       "                                     solver='adam', tol=0.0001,\n",
       "                                     validation_fraction=0.1, verbose=False,\n",
       "                                     warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'activation': ['tanh', 'relu'],\n",
       "                         'alpha': [0.0001, 0.05],\n",
       "                         'hidden_layer_sizes': [(50, 50, 50), (50, 100, 50),\n",
       "                                                (100,)],\n",
       "                         'learning_rate': ['constant', 'adaptive'],\n",
       "                         'solver': ['sgd', 'adam']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(mlp, grid_params, n_jobs=-1, cv=5, verbose=5)\n",
    "clf.fit(inputs,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'tanh',\n",
       " 'alpha': 0.05,\n",
       " 'hidden_layer_sizes': (100,),\n",
       " 'learning_rate': 'adaptive',\n",
       " 'solver': 'sgd'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
