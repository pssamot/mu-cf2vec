{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_vae = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just choose the name of the dataset directory\n",
    "DATA_DIR = '/Users/tomas/Documents/FEUP/Tese/data/ml-20m/processed_70_10_20'\n",
    "if is_vae:\n",
    "    PARSE_DATA_DIR = os.path.join(DATA_DIR, 'embeddings/vae')\n",
    "else:\n",
    "    PARSE_DATA_DIR = os.path.join(DATA_DIR, 'embeddings/cdae')\n",
    "    file = '200_fac_metadataset_k_20.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'metadataset_k_20.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>first_place</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7648</td>\n",
       "      <td>-0.431028</td>\n",
       "      <td>0.273519</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>-0.021366</td>\n",
       "      <td>0.411798</td>\n",
       "      <td>0.445795</td>\n",
       "      <td>1.206461</td>\n",
       "      <td>0.437548</td>\n",
       "      <td>-0.670044</td>\n",
       "      <td>...</td>\n",
       "      <td>1.261934</td>\n",
       "      <td>0.238986</td>\n",
       "      <td>0.150137</td>\n",
       "      <td>-0.024725</td>\n",
       "      <td>-0.349221</td>\n",
       "      <td>3.509393</td>\n",
       "      <td>-0.146421</td>\n",
       "      <td>1.508804</td>\n",
       "      <td>-0.859744</td>\n",
       "      <td>als_ndcg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10208</td>\n",
       "      <td>0.351687</td>\n",
       "      <td>-0.626017</td>\n",
       "      <td>0.034143</td>\n",
       "      <td>-0.080265</td>\n",
       "      <td>0.101129</td>\n",
       "      <td>-0.229498</td>\n",
       "      <td>0.261393</td>\n",
       "      <td>-1.189405</td>\n",
       "      <td>-0.451783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.964082</td>\n",
       "      <td>0.825656</td>\n",
       "      <td>0.064932</td>\n",
       "      <td>0.042978</td>\n",
       "      <td>-1.043557</td>\n",
       "      <td>0.562128</td>\n",
       "      <td>0.234560</td>\n",
       "      <td>0.813003</td>\n",
       "      <td>0.511105</td>\n",
       "      <td>zeroes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13315</td>\n",
       "      <td>-0.706174</td>\n",
       "      <td>-0.031290</td>\n",
       "      <td>-0.005091</td>\n",
       "      <td>-0.097111</td>\n",
       "      <td>0.111871</td>\n",
       "      <td>0.484369</td>\n",
       "      <td>0.737223</td>\n",
       "      <td>-1.132764</td>\n",
       "      <td>1.119018</td>\n",
       "      <td>...</td>\n",
       "      <td>1.040254</td>\n",
       "      <td>-1.375516</td>\n",
       "      <td>-0.064951</td>\n",
       "      <td>0.030435</td>\n",
       "      <td>0.757224</td>\n",
       "      <td>0.444452</td>\n",
       "      <td>0.753969</td>\n",
       "      <td>-0.582505</td>\n",
       "      <td>0.761838</td>\n",
       "      <td>bpr_ndcg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16144</td>\n",
       "      <td>0.672244</td>\n",
       "      <td>-0.958536</td>\n",
       "      <td>-0.005133</td>\n",
       "      <td>-0.093083</td>\n",
       "      <td>0.118219</td>\n",
       "      <td>-0.325690</td>\n",
       "      <td>1.434977</td>\n",
       "      <td>0.006304</td>\n",
       "      <td>-0.780396</td>\n",
       "      <td>...</td>\n",
       "      <td>1.159395</td>\n",
       "      <td>-0.746610</td>\n",
       "      <td>0.042197</td>\n",
       "      <td>0.038217</td>\n",
       "      <td>-0.813577</td>\n",
       "      <td>1.753534</td>\n",
       "      <td>-0.063353</td>\n",
       "      <td>-0.829087</td>\n",
       "      <td>-1.122440</td>\n",
       "      <td>most_popular_ndcg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18064</td>\n",
       "      <td>-0.813108</td>\n",
       "      <td>0.897909</td>\n",
       "      <td>-0.105261</td>\n",
       "      <td>0.080410</td>\n",
       "      <td>0.099298</td>\n",
       "      <td>-1.109625</td>\n",
       "      <td>2.775797</td>\n",
       "      <td>0.139941</td>\n",
       "      <td>-0.745728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.584199</td>\n",
       "      <td>0.619601</td>\n",
       "      <td>0.027197</td>\n",
       "      <td>0.051431</td>\n",
       "      <td>-1.668557</td>\n",
       "      <td>1.609979</td>\n",
       "      <td>-1.117613</td>\n",
       "      <td>1.222671</td>\n",
       "      <td>-2.145877</td>\n",
       "      <td>lmf_ndcg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   original_id         1         2         3         4         5         6  \\\n",
       "0         7648 -0.431028  0.273519  0.000467 -0.021366  0.411798  0.445795   \n",
       "1        10208  0.351687 -0.626017  0.034143 -0.080265  0.101129 -0.229498   \n",
       "2        13315 -0.706174 -0.031290 -0.005091 -0.097111  0.111871  0.484369   \n",
       "3        16144  0.672244 -0.958536 -0.005133 -0.093083  0.118219 -0.325690   \n",
       "4        18064 -0.813108  0.897909 -0.105261  0.080410  0.099298 -1.109625   \n",
       "\n",
       "          7         8         9  ...       192       193       194       195  \\\n",
       "0  1.206461  0.437548 -0.670044  ...  1.261934  0.238986  0.150137 -0.024725   \n",
       "1  0.261393 -1.189405 -0.451783  ...  0.964082  0.825656  0.064932  0.042978   \n",
       "2  0.737223 -1.132764  1.119018  ...  1.040254 -1.375516 -0.064951  0.030435   \n",
       "3  1.434977  0.006304 -0.780396  ...  1.159395 -0.746610  0.042197  0.038217   \n",
       "4  2.775797  0.139941 -0.745728  ...  0.584199  0.619601  0.027197  0.051431   \n",
       "\n",
       "        196       197       198       199       200        first_place  \n",
       "0 -0.349221  3.509393 -0.146421  1.508804 -0.859744           als_ndcg  \n",
       "1 -1.043557  0.562128  0.234560  0.813003  0.511105             zeroes  \n",
       "2  0.757224  0.444452  0.753969 -0.582505  0.761838           bpr_ndcg  \n",
       "3 -0.813577  1.753534 -0.063353 -0.829087 -1.122440  most_popular_ndcg  \n",
       "4 -1.668557  1.609979 -1.117613  1.222671 -2.145877           lmf_ndcg  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the data using pandas\n",
    "metadataset = pd.read_csv(os.path.join(PARSE_DATA_DIR, file ))\n",
    "metadataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "als_ndcg  %  0.2636851961743875\n",
      "most_popular_ndcg  %  0.22417287699314872\n",
      "bpr_ndcg  %  0.19979654276840556\n",
      "zeroes  %  0.16397573926647502\n",
      "lmf_ndcg  %  0.14836964479758316\n"
     ]
    }
   ],
   "source": [
    "total = metadataset.shape[0]\n",
    "names = ['als_ndcg','most_popular_ndcg','bpr_ndcg','zeroes','lmf_ndcg']\n",
    "for name, count in zip(names,metadataset.first_place.value_counts()):\n",
    "    print(name,\" % \",count/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#als:0\n",
    "#bpr:1\n",
    "#lmf:2\n",
    "#most_pop_3\n",
    "#zeros:4\n",
    "target_pre = metadataset['first_place'].values \n",
    "label_encoder = LabelEncoder()\n",
    "target = label_encoder.fit_transform(target_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if normalize:\n",
    "  #---- SET INPUTS -----\n",
    "  scaler = StandardScaler()\n",
    "  #Compute the mean and std to be used for later scaling.\n",
    "  scaler.fit(metadataset.drop(columns=['first_place','original_id']))\n",
    "  # Perform standardization by centering and scaling\n",
    "  inputs_transform = scaler.transform(metadataset.drop(columns=['first_place','original_id']))\n",
    "  inputs = pd.DataFrame(inputs_transform)\n",
    "  inputs.head()\n",
    "else:\n",
    "  inputs = metadataset.drop(columns=['first_place','original_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.015749</td>\n",
       "      <td>-0.015748</td>\n",
       "      <td>-0.01575</td>\n",
       "      <td>-0.015741</td>\n",
       "      <td>-0.015757</td>\n",
       "      <td>-0.015755</td>\n",
       "      <td>-0.01576</td>\n",
       "      <td>-0.015748</td>\n",
       "      <td>-0.015751</td>\n",
       "      <td>-0.015741</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015741</td>\n",
       "      <td>-0.015739</td>\n",
       "      <td>-0.015762</td>\n",
       "      <td>-0.015765</td>\n",
       "      <td>-0.015751</td>\n",
       "      <td>-0.015763</td>\n",
       "      <td>-0.015753</td>\n",
       "      <td>-0.015739</td>\n",
       "      <td>-0.015753</td>\n",
       "      <td>-0.015763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.015749</td>\n",
       "      <td>-0.015748</td>\n",
       "      <td>-0.01575</td>\n",
       "      <td>-0.015741</td>\n",
       "      <td>-0.015757</td>\n",
       "      <td>-0.015755</td>\n",
       "      <td>-0.01576</td>\n",
       "      <td>-0.015748</td>\n",
       "      <td>-0.015751</td>\n",
       "      <td>-0.015741</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015741</td>\n",
       "      <td>-0.015739</td>\n",
       "      <td>-0.015762</td>\n",
       "      <td>-0.015765</td>\n",
       "      <td>-0.015751</td>\n",
       "      <td>-0.015763</td>\n",
       "      <td>-0.015753</td>\n",
       "      <td>-0.015739</td>\n",
       "      <td>-0.015753</td>\n",
       "      <td>-0.015763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.015749</td>\n",
       "      <td>-0.015748</td>\n",
       "      <td>-0.01575</td>\n",
       "      <td>-0.015741</td>\n",
       "      <td>-0.015757</td>\n",
       "      <td>-0.015755</td>\n",
       "      <td>-0.01576</td>\n",
       "      <td>-0.015748</td>\n",
       "      <td>-0.015751</td>\n",
       "      <td>-0.015741</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015741</td>\n",
       "      <td>-0.015739</td>\n",
       "      <td>-0.015762</td>\n",
       "      <td>-0.015765</td>\n",
       "      <td>-0.015751</td>\n",
       "      <td>-0.015763</td>\n",
       "      <td>-0.015753</td>\n",
       "      <td>-0.015739</td>\n",
       "      <td>-0.015753</td>\n",
       "      <td>-0.015763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.015749</td>\n",
       "      <td>-0.015748</td>\n",
       "      <td>-0.01575</td>\n",
       "      <td>-0.015741</td>\n",
       "      <td>-0.015757</td>\n",
       "      <td>-0.015755</td>\n",
       "      <td>-0.01576</td>\n",
       "      <td>-0.015748</td>\n",
       "      <td>-0.015751</td>\n",
       "      <td>-0.015741</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015741</td>\n",
       "      <td>-0.015739</td>\n",
       "      <td>-0.015762</td>\n",
       "      <td>-0.015765</td>\n",
       "      <td>-0.015751</td>\n",
       "      <td>-0.015763</td>\n",
       "      <td>-0.015753</td>\n",
       "      <td>-0.015739</td>\n",
       "      <td>-0.015753</td>\n",
       "      <td>-0.015763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.015749</td>\n",
       "      <td>-0.015748</td>\n",
       "      <td>-0.01575</td>\n",
       "      <td>-0.015741</td>\n",
       "      <td>-0.015757</td>\n",
       "      <td>-0.015755</td>\n",
       "      <td>-0.01576</td>\n",
       "      <td>-0.015748</td>\n",
       "      <td>-0.015751</td>\n",
       "      <td>-0.015741</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015741</td>\n",
       "      <td>-0.015739</td>\n",
       "      <td>-0.015762</td>\n",
       "      <td>-0.015765</td>\n",
       "      <td>-0.015751</td>\n",
       "      <td>-0.015763</td>\n",
       "      <td>-0.015753</td>\n",
       "      <td>-0.015739</td>\n",
       "      <td>-0.015753</td>\n",
       "      <td>-0.015763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1        2         3         4         5        6    \\\n",
       "0 -0.015749 -0.015748 -0.01575 -0.015741 -0.015757 -0.015755 -0.01576   \n",
       "1 -0.015749 -0.015748 -0.01575 -0.015741 -0.015757 -0.015755 -0.01576   \n",
       "2 -0.015749 -0.015748 -0.01575 -0.015741 -0.015757 -0.015755 -0.01576   \n",
       "3 -0.015749 -0.015748 -0.01575 -0.015741 -0.015757 -0.015755 -0.01576   \n",
       "4 -0.015749 -0.015748 -0.01575 -0.015741 -0.015757 -0.015755 -0.01576   \n",
       "\n",
       "        7         8         9    ...       190       191       192       193  \\\n",
       "0 -0.015748 -0.015751 -0.015741  ... -0.015741 -0.015739 -0.015762 -0.015765   \n",
       "1 -0.015748 -0.015751 -0.015741  ... -0.015741 -0.015739 -0.015762 -0.015765   \n",
       "2 -0.015748 -0.015751 -0.015741  ... -0.015741 -0.015739 -0.015762 -0.015765   \n",
       "3 -0.015748 -0.015751 -0.015741  ... -0.015741 -0.015739 -0.015762 -0.015765   \n",
       "4 -0.015748 -0.015751 -0.015741  ... -0.015741 -0.015739 -0.015762 -0.015765   \n",
       "\n",
       "        194       195       196       197       198       199  \n",
       "0 -0.015751 -0.015763 -0.015753 -0.015739 -0.015753 -0.015763  \n",
       "1 -0.015751 -0.015763 -0.015753 -0.015739 -0.015753 -0.015763  \n",
       "2 -0.015751 -0.015763 -0.015753 -0.015739 -0.015753 -0.015763  \n",
       "3 -0.015751 -0.015763 -0.015753 -0.015739 -0.015753 -0.015763  \n",
       "4 -0.015751 -0.015763 -0.015753 -0.015739 -0.015753 -0.015763  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits()\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'activation': 'relu',\n",
    "    'alpha': 0.05,\n",
    "    'hidden_layer_sizes': (50, 50, 50),\n",
    "    'learning_rate': 'constant',\n",
    "    'solver': 'sgd'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  1\n",
      "fit\n",
      "Iteration 1, loss = 1.64094226\n",
      "Iteration 2, loss = 1.59352365\n",
      "Iteration 3, loss = 1.58078347\n",
      "Iteration 4, loss = 1.57367491\n",
      "Iteration 5, loss = 1.56865523\n",
      "Iteration 6, loss = 1.56493532\n",
      "Iteration 7, loss = 1.56191316\n",
      "Iteration 8, loss = 1.55955003\n",
      "Iteration 9, loss = 1.55749718\n",
      "Iteration 10, loss = 1.55570656\n",
      "Iteration 11, loss = 1.55416540\n",
      "Iteration 12, loss = 1.55279532\n",
      "Iteration 13, loss = 1.55162111\n",
      "Iteration 14, loss = 1.55043579\n",
      "Iteration 15, loss = 1.54934860\n",
      "Iteration 16, loss = 1.54837302\n",
      "Iteration 17, loss = 1.54739995\n",
      "Iteration 18, loss = 1.54650007\n",
      "Iteration 19, loss = 1.54558750\n",
      "Iteration 20, loss = 1.54476357\n",
      "Iteration 21, loss = 1.54397334\n",
      "Iteration 22, loss = 1.54320044\n",
      "Iteration 23, loss = 1.54240633\n",
      "Iteration 24, loss = 1.54166821\n",
      "Iteration 25, loss = 1.54094289\n",
      "Iteration 26, loss = 1.54021464\n",
      "Iteration 27, loss = 1.53959890\n",
      "Iteration 28, loss = 1.53883797\n",
      "Iteration 29, loss = 1.53820443\n",
      "Iteration 30, loss = 1.53756573\n",
      "Iteration 31, loss = 1.53692422\n",
      "Iteration 32, loss = 1.53628856\n",
      "Iteration 33, loss = 1.53567153\n",
      "Iteration 34, loss = 1.53509103\n",
      "Iteration 35, loss = 1.53453351\n",
      "Iteration 36, loss = 1.53395685\n",
      "Iteration 37, loss = 1.53342317\n",
      "Iteration 38, loss = 1.53280977\n",
      "Iteration 39, loss = 1.53223433\n",
      "Iteration 40, loss = 1.53168667\n",
      "Iteration 41, loss = 1.53115020\n",
      "Iteration 42, loss = 1.53064587\n",
      "Iteration 43, loss = 1.53003903\n",
      "Iteration 44, loss = 1.52954534\n",
      "Iteration 45, loss = 1.52900682\n",
      "Iteration 46, loss = 1.52853659\n",
      "Iteration 47, loss = 1.52804427\n",
      "Iteration 48, loss = 1.52744088\n",
      "Iteration 49, loss = 1.52698184\n",
      "Iteration 50, loss = 1.52648491\n",
      "Iteration 51, loss = 1.52597637\n",
      "Iteration 52, loss = 1.52552871\n",
      "Iteration 53, loss = 1.52500190\n",
      "Iteration 54, loss = 1.52457083\n",
      "Iteration 55, loss = 1.52406377\n",
      "Iteration 56, loss = 1.52347944\n",
      "Iteration 57, loss = 1.52315788\n",
      "Iteration 58, loss = 1.52260230\n",
      "Iteration 59, loss = 1.52213033\n",
      "Iteration 60, loss = 1.52168721\n",
      "Iteration 61, loss = 1.52120578\n",
      "Iteration 62, loss = 1.52075436\n",
      "Iteration 63, loss = 1.52035016\n",
      "Iteration 64, loss = 1.51991013\n",
      "Iteration 65, loss = 1.51935919\n",
      "Iteration 66, loss = 1.51898993\n",
      "Iteration 67, loss = 1.51861619\n",
      "Iteration 68, loss = 1.51810149\n",
      "Iteration 69, loss = 1.51773225\n",
      "Iteration 70, loss = 1.51729509\n",
      "Iteration 71, loss = 1.51672910\n",
      "Iteration 72, loss = 1.51638658\n",
      "Iteration 73, loss = 1.51595757\n",
      "Iteration 74, loss = 1.51556751\n",
      "Iteration 75, loss = 1.51512382\n",
      "Iteration 76, loss = 1.51468457\n",
      "Iteration 77, loss = 1.51424511\n",
      "Iteration 78, loss = 1.51381405\n",
      "Iteration 79, loss = 1.51337987\n",
      "Iteration 80, loss = 1.51300281\n",
      "Iteration 81, loss = 1.51264888\n",
      "Iteration 82, loss = 1.51219733\n",
      "Iteration 83, loss = 1.51182287\n",
      "Iteration 84, loss = 1.51147141\n",
      "Iteration 85, loss = 1.51101011\n",
      "Iteration 86, loss = 1.51056786\n",
      "Iteration 87, loss = 1.51021955\n",
      "Iteration 88, loss = 1.50981115\n",
      "Iteration 89, loss = 1.50938113\n",
      "Iteration 90, loss = 1.50902867\n",
      "Iteration 91, loss = 1.50855237\n",
      "Iteration 92, loss = 1.50827130\n",
      "Iteration 93, loss = 1.50785616\n",
      "Iteration 94, loss = 1.50751497\n",
      "Iteration 95, loss = 1.50717102\n",
      "Iteration 96, loss = 1.50676891\n",
      "Iteration 97, loss = 1.50638574\n",
      "Iteration 98, loss = 1.50587539\n",
      "Iteration 99, loss = 1.50557987\n",
      "Iteration 100, loss = 1.50522959\n",
      "Iteration 101, loss = 1.50486495\n",
      "Iteration 102, loss = 1.50446745\n",
      "Iteration 103, loss = 1.50410304\n",
      "Iteration 104, loss = 1.50373978\n",
      "Iteration 105, loss = 1.50338106\n",
      "Iteration 106, loss = 1.50299839\n",
      "Iteration 107, loss = 1.50266029\n",
      "Iteration 108, loss = 1.50218876\n",
      "Iteration 109, loss = 1.50192241\n",
      "Iteration 110, loss = 1.50147966\n",
      "Iteration 111, loss = 1.50114185\n",
      "Iteration 112, loss = 1.50081368\n",
      "Iteration 113, loss = 1.50041060\n",
      "Iteration 114, loss = 1.50009806\n",
      "Iteration 115, loss = 1.49970508\n",
      "Iteration 116, loss = 1.49931624\n",
      "Iteration 117, loss = 1.49883725\n",
      "Iteration 118, loss = 1.49856853\n",
      "Iteration 119, loss = 1.49827266\n",
      "Iteration 120, loss = 1.49797560\n",
      "Iteration 121, loss = 1.49757300\n",
      "Iteration 122, loss = 1.49721298\n",
      "Iteration 123, loss = 1.49677791\n",
      "Iteration 124, loss = 1.49644051\n",
      "Iteration 125, loss = 1.49619472\n",
      "Iteration 126, loss = 1.49566309\n",
      "Iteration 127, loss = 1.49551930\n",
      "Iteration 128, loss = 1.49498589\n",
      "Iteration 129, loss = 1.49459404\n",
      "Iteration 130, loss = 1.49435406\n",
      "Iteration 131, loss = 1.49402100\n",
      "Iteration 132, loss = 1.49361736\n",
      "Iteration 133, loss = 1.49328749\n",
      "Iteration 134, loss = 1.49301960\n",
      "Iteration 135, loss = 1.49271304\n",
      "Iteration 136, loss = 1.49245405\n",
      "Iteration 137, loss = 1.49195521\n",
      "Iteration 138, loss = 1.49167850\n",
      "Iteration 139, loss = 1.49130129\n",
      "Iteration 140, loss = 1.49091072\n",
      "Iteration 141, loss = 1.49055383\n",
      "Iteration 142, loss = 1.49036574\n",
      "Iteration 143, loss = 1.48989323\n",
      "Iteration 144, loss = 1.48965425\n",
      "Iteration 145, loss = 1.48929476\n",
      "Iteration 146, loss = 1.48897536\n",
      "Iteration 147, loss = 1.48858999\n",
      "Iteration 148, loss = 1.48821755\n",
      "Iteration 149, loss = 1.48785717\n",
      "Iteration 150, loss = 1.48761747\n",
      "Iteration 151, loss = 1.48742582\n",
      "Iteration 152, loss = 1.48696456\n",
      "Iteration 153, loss = 1.48666549\n",
      "Iteration 154, loss = 1.48623448\n",
      "Iteration 155, loss = 1.48581747\n",
      "Iteration 156, loss = 1.48564617\n",
      "Iteration 157, loss = 1.48528380\n",
      "Iteration 158, loss = 1.48492349\n",
      "Iteration 159, loss = 1.48454631\n",
      "Iteration 160, loss = 1.48428182\n",
      "Iteration 161, loss = 1.48400684\n",
      "Iteration 162, loss = 1.48370615\n",
      "Iteration 163, loss = 1.48327747\n",
      "Iteration 164, loss = 1.48308225\n",
      "Iteration 165, loss = 1.48260622\n",
      "Iteration 166, loss = 1.48235055\n",
      "Iteration 167, loss = 1.48192789\n",
      "Iteration 168, loss = 1.48175431\n",
      "Iteration 169, loss = 1.48147379\n",
      "Iteration 170, loss = 1.48104853\n",
      "Iteration 171, loss = 1.48076280\n",
      "Iteration 172, loss = 1.48064028\n",
      "Iteration 173, loss = 1.48018705\n",
      "Iteration 174, loss = 1.47995131\n",
      "Iteration 175, loss = 1.47964791\n",
      "Iteration 176, loss = 1.47920532\n",
      "Iteration 177, loss = 1.47895706\n",
      "Iteration 178, loss = 1.47875376\n",
      "Iteration 179, loss = 1.47823382\n",
      "Iteration 180, loss = 1.47819641\n",
      "Iteration 181, loss = 1.47790854\n",
      "Iteration 182, loss = 1.47753921\n",
      "Iteration 183, loss = 1.47719165\n",
      "Iteration 184, loss = 1.47669322\n",
      "Iteration 185, loss = 1.47661726\n",
      "Iteration 186, loss = 1.47621030\n",
      "Iteration 187, loss = 1.47588677\n",
      "Iteration 188, loss = 1.47558208\n",
      "Iteration 189, loss = 1.47538688\n",
      "Iteration 190, loss = 1.47488959\n",
      "Iteration 191, loss = 1.47487006\n",
      "Iteration 192, loss = 1.47442612\n",
      "Iteration 193, loss = 1.47428585\n",
      "Iteration 194, loss = 1.47380348\n",
      "Iteration 195, loss = 1.47347433\n",
      "Iteration 196, loss = 1.47334335\n",
      "Iteration 197, loss = 1.47305115\n",
      "Iteration 198, loss = 1.47258892\n",
      "Iteration 199, loss = 1.47251192\n",
      "Iteration 200, loss = 1.47215651\n",
      "Iteration 201, loss = 1.47190099\n",
      "Iteration 202, loss = 1.47158127\n",
      "Iteration 203, loss = 1.47123506\n",
      "Iteration 204, loss = 1.47109389\n",
      "Iteration 205, loss = 1.47090860\n",
      "Iteration 206, loss = 1.47031187\n",
      "Iteration 207, loss = 1.47009354\n",
      "Iteration 208, loss = 1.46980507\n",
      "Iteration 209, loss = 1.46981140\n",
      "Iteration 210, loss = 1.46924712\n",
      "Iteration 211, loss = 1.46927138\n",
      "Iteration 212, loss = 1.46887213\n",
      "Iteration 213, loss = 1.46844651\n",
      "Iteration 214, loss = 1.46822592\n",
      "Iteration 215, loss = 1.46812681\n",
      "Iteration 216, loss = 1.46770255\n",
      "Iteration 217, loss = 1.46731512\n",
      "Iteration 218, loss = 1.46725455\n",
      "Iteration 219, loss = 1.46707683\n",
      "Iteration 220, loss = 1.46654811\n",
      "Iteration 221, loss = 1.46658822\n",
      "Iteration 222, loss = 1.46624034\n",
      "Iteration 223, loss = 1.46583355\n",
      "Iteration 224, loss = 1.46561215\n",
      "Iteration 225, loss = 1.46543096\n",
      "Iteration 226, loss = 1.46504578\n",
      "Iteration 227, loss = 1.46487203\n",
      "Iteration 228, loss = 1.46471261\n",
      "Iteration 229, loss = 1.46427683\n",
      "Iteration 230, loss = 1.46418432\n",
      "Iteration 231, loss = 1.46393151\n",
      "Iteration 232, loss = 1.46356607\n",
      "Iteration 233, loss = 1.46351295\n",
      "Iteration 234, loss = 1.46293876\n",
      "Iteration 235, loss = 1.46262844\n",
      "Iteration 236, loss = 1.46283946\n",
      "Iteration 237, loss = 1.46242811\n",
      "Iteration 238, loss = 1.46212798\n",
      "Iteration 239, loss = 1.46199512\n",
      "Iteration 240, loss = 1.46155467\n",
      "Iteration 241, loss = 1.46129309\n",
      "Iteration 242, loss = 1.46122120\n",
      "Iteration 243, loss = 1.46089438\n",
      "Iteration 244, loss = 1.46065130\n",
      "Iteration 245, loss = 1.46046657\n",
      "Iteration 246, loss = 1.46016165\n",
      "Iteration 247, loss = 1.45998767\n",
      "Iteration 248, loss = 1.45971576\n",
      "Iteration 249, loss = 1.45953804\n",
      "Iteration 250, loss = 1.45928791\n",
      "Iteration 251, loss = 1.45901164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 252, loss = 1.45872335\n",
      "Iteration 253, loss = 1.45850379\n",
      "Iteration 254, loss = 1.45828481\n",
      "Iteration 255, loss = 1.45820408\n",
      "Iteration 256, loss = 1.45788461\n",
      "Iteration 257, loss = 1.45765557\n",
      "Iteration 258, loss = 1.45739741\n",
      "Iteration 259, loss = 1.45748050\n",
      "Iteration 260, loss = 1.45688004\n",
      "Iteration 261, loss = 1.45652499\n",
      "Iteration 262, loss = 1.45657397\n",
      "Iteration 263, loss = 1.45632464\n",
      "Iteration 264, loss = 1.45607886\n",
      "Iteration 265, loss = 1.45575443\n",
      "Iteration 266, loss = 1.45561159\n",
      "Iteration 267, loss = 1.45542580\n",
      "Iteration 268, loss = 1.45531489\n",
      "Iteration 269, loss = 1.45478857\n",
      "Iteration 270, loss = 1.45496607\n",
      "Iteration 271, loss = 1.45444681\n",
      "Iteration 272, loss = 1.45422215\n",
      "Iteration 273, loss = 1.45403069\n",
      "Iteration 274, loss = 1.45395992\n",
      "Iteration 275, loss = 1.45370688\n",
      "Iteration 276, loss = 1.45335714\n",
      "Iteration 277, loss = 1.45341371\n",
      "Iteration 278, loss = 1.45305658\n",
      "Iteration 279, loss = 1.45270799\n",
      "Iteration 280, loss = 1.45244556\n",
      "Iteration 281, loss = 1.45239499\n",
      "Iteration 282, loss = 1.45229290\n",
      "Iteration 283, loss = 1.45225245\n",
      "Iteration 284, loss = 1.45205114\n",
      "Iteration 285, loss = 1.45163575\n",
      "Iteration 286, loss = 1.45136821\n",
      "Iteration 287, loss = 1.45121734\n",
      "Iteration 288, loss = 1.45086702\n",
      "Iteration 289, loss = 1.45113572\n",
      "Iteration 290, loss = 1.45051726\n",
      "Iteration 291, loss = 1.45051900\n",
      "Iteration 292, loss = 1.44985139\n",
      "Iteration 293, loss = 1.45021956\n",
      "Iteration 294, loss = 1.44983171\n",
      "Iteration 295, loss = 1.44954855\n",
      "Iteration 296, loss = 1.44956980\n",
      "Iteration 297, loss = 1.44926840\n",
      "Iteration 298, loss = 1.44897913\n",
      "Iteration 299, loss = 1.44867559\n",
      "Iteration 300, loss = 1.44865439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end:  1\n",
      "iteration:  2\n",
      "fit\n",
      "Iteration 1, loss = 1.64035425\n",
      "Iteration 2, loss = 1.59249285\n",
      "Iteration 3, loss = 1.57949209\n",
      "Iteration 4, loss = 1.57222112\n",
      "Iteration 5, loss = 1.56716887\n",
      "Iteration 6, loss = 1.56329210\n",
      "Iteration 7, loss = 1.56027774\n",
      "Iteration 8, loss = 1.55782426\n",
      "Iteration 9, loss = 1.55574794\n",
      "Iteration 10, loss = 1.55396463\n",
      "Iteration 11, loss = 1.55244063\n",
      "Iteration 12, loss = 1.55103555\n",
      "Iteration 13, loss = 1.54982536\n",
      "Iteration 14, loss = 1.54866094\n",
      "Iteration 15, loss = 1.54756037\n",
      "Iteration 16, loss = 1.54660486\n",
      "Iteration 17, loss = 1.54559315\n",
      "Iteration 18, loss = 1.54468867\n",
      "Iteration 19, loss = 1.54380334\n",
      "Iteration 20, loss = 1.54294092\n",
      "Iteration 21, loss = 1.54211688\n",
      "Iteration 22, loss = 1.54131899\n",
      "Iteration 23, loss = 1.54053700\n",
      "Iteration 24, loss = 1.53978993\n",
      "Iteration 25, loss = 1.53901263\n",
      "Iteration 26, loss = 1.53830101\n",
      "Iteration 27, loss = 1.53760753\n",
      "Iteration 28, loss = 1.53688160\n",
      "Iteration 29, loss = 1.53617868\n",
      "Iteration 30, loss = 1.53556774\n",
      "Iteration 31, loss = 1.53488990\n",
      "Iteration 32, loss = 1.53425214\n",
      "Iteration 33, loss = 1.53359709\n",
      "Iteration 34, loss = 1.53299843\n",
      "Iteration 35, loss = 1.53243293\n",
      "Iteration 36, loss = 1.53179770\n",
      "Iteration 37, loss = 1.53121175\n",
      "Iteration 38, loss = 1.53060971\n",
      "Iteration 39, loss = 1.53003583\n",
      "Iteration 40, loss = 1.52949351\n",
      "Iteration 41, loss = 1.52901763\n",
      "Iteration 42, loss = 1.52840461\n",
      "Iteration 43, loss = 1.52786846\n",
      "Iteration 44, loss = 1.52729801\n",
      "Iteration 45, loss = 1.52673548\n",
      "Iteration 46, loss = 1.52626089\n",
      "Iteration 47, loss = 1.52573224\n",
      "Iteration 48, loss = 1.52521016\n",
      "Iteration 49, loss = 1.52469119\n",
      "Iteration 50, loss = 1.52421434\n",
      "Iteration 51, loss = 1.52369522\n",
      "Iteration 52, loss = 1.52325697\n",
      "Iteration 53, loss = 1.52271692\n",
      "Iteration 54, loss = 1.52223615\n",
      "Iteration 55, loss = 1.52177841\n",
      "Iteration 56, loss = 1.52120683\n",
      "Iteration 57, loss = 1.52086785\n",
      "Iteration 58, loss = 1.52028662\n",
      "Iteration 59, loss = 1.51986442\n",
      "Iteration 60, loss = 1.51945132\n",
      "Iteration 61, loss = 1.51894428\n",
      "Iteration 62, loss = 1.51845044\n",
      "Iteration 63, loss = 1.51804915\n",
      "Iteration 64, loss = 1.51764982\n",
      "Iteration 65, loss = 1.51710504\n",
      "Iteration 66, loss = 1.51672306\n",
      "Iteration 67, loss = 1.51634740\n",
      "Iteration 68, loss = 1.51593773\n",
      "Iteration 69, loss = 1.51548062\n",
      "Iteration 70, loss = 1.51507258\n",
      "Iteration 71, loss = 1.51459213\n",
      "Iteration 72, loss = 1.51422634\n",
      "Iteration 73, loss = 1.51380733\n",
      "Iteration 74, loss = 1.51348892\n",
      "Iteration 75, loss = 1.51298645\n",
      "Iteration 76, loss = 1.51260046\n",
      "Iteration 77, loss = 1.51219724\n",
      "Iteration 78, loss = 1.51175577\n",
      "Iteration 79, loss = 1.51133139\n",
      "Iteration 80, loss = 1.51104120\n",
      "Iteration 81, loss = 1.51062956\n",
      "Iteration 82, loss = 1.51020176\n",
      "Iteration 83, loss = 1.50986263\n",
      "Iteration 84, loss = 1.50949997\n",
      "Iteration 85, loss = 1.50910341\n",
      "Iteration 86, loss = 1.50866694\n",
      "Iteration 87, loss = 1.50830581\n",
      "Iteration 88, loss = 1.50799529\n",
      "Iteration 89, loss = 1.50755350\n",
      "Iteration 90, loss = 1.50724446\n",
      "Iteration 91, loss = 1.50682126\n",
      "Iteration 92, loss = 1.50646980\n",
      "Iteration 93, loss = 1.50608717\n",
      "Iteration 94, loss = 1.50570166\n",
      "Iteration 95, loss = 1.50540426\n",
      "Iteration 96, loss = 1.50501564\n",
      "Iteration 97, loss = 1.50464284\n",
      "Iteration 98, loss = 1.50421271\n",
      "Iteration 99, loss = 1.50392858\n",
      "Iteration 100, loss = 1.50360023\n",
      "Iteration 101, loss = 1.50319992\n",
      "Iteration 102, loss = 1.50280417\n",
      "Iteration 103, loss = 1.50249056\n",
      "Iteration 104, loss = 1.50207405\n",
      "Iteration 105, loss = 1.50180842\n",
      "Iteration 106, loss = 1.50132227\n",
      "Iteration 107, loss = 1.50110952\n",
      "Iteration 108, loss = 1.50063874\n",
      "Iteration 109, loss = 1.50037265\n",
      "Iteration 110, loss = 1.49996632\n",
      "Iteration 111, loss = 1.49957713\n",
      "Iteration 112, loss = 1.49924792\n",
      "Iteration 113, loss = 1.49888140\n",
      "Iteration 114, loss = 1.49860429\n",
      "Iteration 115, loss = 1.49818912\n",
      "Iteration 116, loss = 1.49779953\n",
      "Iteration 117, loss = 1.49737234\n",
      "Iteration 118, loss = 1.49706416\n",
      "Iteration 119, loss = 1.49696177\n",
      "Iteration 120, loss = 1.49646786\n",
      "Iteration 121, loss = 1.49607820\n",
      "Iteration 122, loss = 1.49575333\n",
      "Iteration 123, loss = 1.49534586\n",
      "Iteration 124, loss = 1.49499964\n",
      "Iteration 125, loss = 1.49475946\n",
      "Iteration 126, loss = 1.49426518\n",
      "Iteration 127, loss = 1.49402582\n",
      "Iteration 128, loss = 1.49362002\n",
      "Iteration 129, loss = 1.49326119\n",
      "Iteration 130, loss = 1.49297019\n",
      "Iteration 131, loss = 1.49262459\n",
      "Iteration 132, loss = 1.49227119\n",
      "Iteration 133, loss = 1.49185539\n",
      "Iteration 134, loss = 1.49162978\n",
      "Iteration 135, loss = 1.49129018\n",
      "Iteration 136, loss = 1.49104998\n",
      "Iteration 137, loss = 1.49054569\n",
      "Iteration 138, loss = 1.49019157\n",
      "Iteration 139, loss = 1.48993405\n",
      "Iteration 140, loss = 1.48953983\n",
      "Iteration 141, loss = 1.48922225\n",
      "Iteration 142, loss = 1.48900110\n",
      "Iteration 143, loss = 1.48858980\n",
      "Iteration 144, loss = 1.48826720\n",
      "Iteration 145, loss = 1.48790606\n",
      "Iteration 146, loss = 1.48766469\n",
      "Iteration 147, loss = 1.48727320\n",
      "Iteration 148, loss = 1.48686412\n",
      "Iteration 149, loss = 1.48653482\n",
      "Iteration 150, loss = 1.48624761\n",
      "Iteration 151, loss = 1.48602303\n",
      "Iteration 152, loss = 1.48556176\n",
      "Iteration 153, loss = 1.48527622\n",
      "Iteration 154, loss = 1.48491774\n",
      "Iteration 155, loss = 1.48451368\n",
      "Iteration 156, loss = 1.48423847\n",
      "Iteration 157, loss = 1.48398977\n",
      "Iteration 158, loss = 1.48356231\n",
      "Iteration 159, loss = 1.48310205\n",
      "Iteration 160, loss = 1.48298701\n",
      "Iteration 161, loss = 1.48271048\n",
      "Iteration 162, loss = 1.48229900\n",
      "Iteration 163, loss = 1.48191728\n",
      "Iteration 164, loss = 1.48159252\n",
      "Iteration 165, loss = 1.48133862\n",
      "Iteration 166, loss = 1.48100839\n",
      "Iteration 167, loss = 1.48073281\n",
      "Iteration 168, loss = 1.48050477\n",
      "Iteration 169, loss = 1.48015184\n",
      "Iteration 170, loss = 1.47968923\n",
      "Iteration 171, loss = 1.47953816\n",
      "Iteration 172, loss = 1.47915370\n",
      "Iteration 173, loss = 1.47871813\n",
      "Iteration 174, loss = 1.47860513\n",
      "Iteration 175, loss = 1.47824423\n",
      "Iteration 176, loss = 1.47792705\n",
      "Iteration 177, loss = 1.47756887\n",
      "Iteration 178, loss = 1.47734061\n",
      "Iteration 179, loss = 1.47687717\n",
      "Iteration 180, loss = 1.47677184\n",
      "Iteration 181, loss = 1.47650542\n",
      "Iteration 182, loss = 1.47609879\n",
      "Iteration 183, loss = 1.47584604\n",
      "Iteration 184, loss = 1.47544240\n",
      "Iteration 185, loss = 1.47515780\n",
      "Iteration 186, loss = 1.47481615\n",
      "Iteration 187, loss = 1.47461679\n",
      "Iteration 188, loss = 1.47429033\n",
      "Iteration 189, loss = 1.47415341\n",
      "Iteration 190, loss = 1.47355892\n",
      "Iteration 191, loss = 1.47347631\n",
      "Iteration 192, loss = 1.47309225\n",
      "Iteration 193, loss = 1.47297608\n",
      "Iteration 194, loss = 1.47244256\n",
      "Iteration 195, loss = 1.47223623\n",
      "Iteration 196, loss = 1.47192410\n",
      "Iteration 197, loss = 1.47165238\n",
      "Iteration 198, loss = 1.47116805\n",
      "Iteration 199, loss = 1.47121351\n",
      "Iteration 200, loss = 1.47073197\n",
      "Iteration 201, loss = 1.47049442\n",
      "Iteration 202, loss = 1.47018004\n",
      "Iteration 203, loss = 1.46972130\n",
      "Iteration 204, loss = 1.46959385\n",
      "Iteration 205, loss = 1.46936046\n",
      "Iteration 206, loss = 1.46896330\n",
      "Iteration 207, loss = 1.46870872\n",
      "Iteration 208, loss = 1.46837882\n",
      "Iteration 209, loss = 1.46813176\n",
      "Iteration 210, loss = 1.46774449\n",
      "Iteration 211, loss = 1.46774678\n",
      "Iteration 212, loss = 1.46750328\n",
      "Iteration 213, loss = 1.46706489\n",
      "Iteration 214, loss = 1.46684666\n",
      "Iteration 215, loss = 1.46648769\n",
      "Iteration 216, loss = 1.46626227\n",
      "Iteration 217, loss = 1.46601937\n",
      "Iteration 218, loss = 1.46579332\n",
      "Iteration 219, loss = 1.46565515\n",
      "Iteration 220, loss = 1.46511960\n",
      "Iteration 221, loss = 1.46497586\n",
      "Iteration 222, loss = 1.46477440\n",
      "Iteration 223, loss = 1.46441580\n",
      "Iteration 224, loss = 1.46395912\n",
      "Iteration 225, loss = 1.46395638\n",
      "Iteration 226, loss = 1.46365013\n",
      "Iteration 227, loss = 1.46329921\n",
      "Iteration 228, loss = 1.46311479\n",
      "Iteration 229, loss = 1.46273037\n",
      "Iteration 230, loss = 1.46269249\n",
      "Iteration 231, loss = 1.46233115\n",
      "Iteration 232, loss = 1.46215082\n",
      "Iteration 233, loss = 1.46180297\n",
      "Iteration 234, loss = 1.46164907\n",
      "Iteration 235, loss = 1.46103699\n",
      "Iteration 236, loss = 1.46123442\n",
      "Iteration 237, loss = 1.46086431\n",
      "Iteration 238, loss = 1.46056862\n",
      "Iteration 239, loss = 1.46021753\n",
      "Iteration 240, loss = 1.46016360\n",
      "Iteration 241, loss = 1.45983236\n",
      "Iteration 242, loss = 1.45962703\n",
      "Iteration 243, loss = 1.45946068\n",
      "Iteration 244, loss = 1.45920173\n",
      "Iteration 245, loss = 1.45895325\n",
      "Iteration 246, loss = 1.45851850\n",
      "Iteration 247, loss = 1.45851091\n",
      "Iteration 248, loss = 1.45820623\n",
      "Iteration 249, loss = 1.45792235\n",
      "Iteration 250, loss = 1.45758957\n",
      "Iteration 251, loss = 1.45742919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 252, loss = 1.45728550\n",
      "Iteration 253, loss = 1.45707721\n",
      "Iteration 254, loss = 1.45671728\n",
      "Iteration 255, loss = 1.45661174\n",
      "Iteration 256, loss = 1.45640092\n",
      "Iteration 257, loss = 1.45621994\n",
      "Iteration 258, loss = 1.45572088\n",
      "Iteration 259, loss = 1.45586219\n",
      "Iteration 260, loss = 1.45537165\n",
      "Iteration 261, loss = 1.45495138\n",
      "Iteration 262, loss = 1.45488720\n",
      "Iteration 263, loss = 1.45466338\n",
      "Iteration 264, loss = 1.45439818\n",
      "Iteration 265, loss = 1.45424889\n",
      "Iteration 266, loss = 1.45409296\n",
      "Iteration 267, loss = 1.45389215\n",
      "Iteration 268, loss = 1.45354723\n",
      "Iteration 269, loss = 1.45320860\n",
      "Iteration 270, loss = 1.45321982\n",
      "Iteration 271, loss = 1.45297282\n",
      "Iteration 272, loss = 1.45248011\n",
      "Iteration 273, loss = 1.45255776\n",
      "Iteration 274, loss = 1.45226399\n",
      "Iteration 275, loss = 1.45202320\n",
      "Iteration 276, loss = 1.45205584\n",
      "Iteration 277, loss = 1.45176526\n",
      "Iteration 278, loss = 1.45153310\n",
      "Iteration 279, loss = 1.45104987\n",
      "Iteration 280, loss = 1.45071807\n",
      "Iteration 281, loss = 1.45094437\n",
      "Iteration 282, loss = 1.45050113\n",
      "Iteration 283, loss = 1.45050198\n",
      "Iteration 284, loss = 1.45034639\n",
      "Iteration 285, loss = 1.45007270\n",
      "Iteration 286, loss = 1.44974971\n",
      "Iteration 287, loss = 1.44979345\n",
      "Iteration 288, loss = 1.44918410\n",
      "Iteration 289, loss = 1.44940309\n",
      "Iteration 290, loss = 1.44888964\n",
      "Iteration 291, loss = 1.44874167\n",
      "Iteration 292, loss = 1.44835073\n",
      "Iteration 293, loss = 1.44833566\n",
      "Iteration 294, loss = 1.44832242\n",
      "Iteration 295, loss = 1.44797158\n",
      "Iteration 296, loss = 1.44800828\n",
      "Iteration 297, loss = 1.44754717\n",
      "Iteration 298, loss = 1.44721877\n",
      "Iteration 299, loss = 1.44705628\n",
      "Iteration 300, loss = 1.44694838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end:  2\n",
      "iteration:  3\n",
      "fit\n",
      "Iteration 1, loss = 1.64120337\n",
      "Iteration 2, loss = 1.59345031\n",
      "Iteration 3, loss = 1.58087815\n",
      "Iteration 4, loss = 1.57371833\n",
      "Iteration 5, loss = 1.56876631\n",
      "Iteration 6, loss = 1.56498387\n",
      "Iteration 7, loss = 1.56196259\n",
      "Iteration 8, loss = 1.55946296\n",
      "Iteration 9, loss = 1.55741984\n",
      "Iteration 10, loss = 1.55568952\n",
      "Iteration 11, loss = 1.55406389\n",
      "Iteration 12, loss = 1.55270923\n",
      "Iteration 13, loss = 1.55145028\n",
      "Iteration 14, loss = 1.55027832\n",
      "Iteration 15, loss = 1.54924685\n",
      "Iteration 16, loss = 1.54819190\n",
      "Iteration 17, loss = 1.54724628\n",
      "Iteration 18, loss = 1.54628816\n",
      "Iteration 19, loss = 1.54545690\n",
      "Iteration 20, loss = 1.54454294\n",
      "Iteration 21, loss = 1.54369000\n",
      "Iteration 22, loss = 1.54290913\n",
      "Iteration 23, loss = 1.54217001\n",
      "Iteration 24, loss = 1.54141382\n",
      "Iteration 25, loss = 1.54061821\n",
      "Iteration 26, loss = 1.53989777\n",
      "Iteration 27, loss = 1.53920606\n",
      "Iteration 28, loss = 1.53852977\n",
      "Iteration 29, loss = 1.53784693\n",
      "Iteration 30, loss = 1.53719939\n",
      "Iteration 31, loss = 1.53654321\n",
      "Iteration 32, loss = 1.53597146\n",
      "Iteration 33, loss = 1.53530604\n",
      "Iteration 34, loss = 1.53464308\n",
      "Iteration 35, loss = 1.53408685\n",
      "Iteration 36, loss = 1.53356768\n",
      "Iteration 37, loss = 1.53294504\n",
      "Iteration 38, loss = 1.53228077\n",
      "Iteration 39, loss = 1.53179481\n",
      "Iteration 40, loss = 1.53118837\n",
      "Iteration 41, loss = 1.53062147\n",
      "Iteration 42, loss = 1.53007348\n",
      "Iteration 43, loss = 1.52950017\n",
      "Iteration 44, loss = 1.52895611\n",
      "Iteration 45, loss = 1.52839485\n",
      "Iteration 46, loss = 1.52789716\n",
      "Iteration 47, loss = 1.52735653\n",
      "Iteration 48, loss = 1.52679355\n",
      "Iteration 49, loss = 1.52631610\n",
      "Iteration 50, loss = 1.52569647\n",
      "Iteration 51, loss = 1.52511938\n",
      "Iteration 52, loss = 1.52475635\n",
      "Iteration 53, loss = 1.52419866\n",
      "Iteration 54, loss = 1.52364091\n",
      "Iteration 55, loss = 1.52321162\n",
      "Iteration 56, loss = 1.52268244\n",
      "Iteration 57, loss = 1.52218089\n",
      "Iteration 58, loss = 1.52166803\n",
      "Iteration 59, loss = 1.52123505\n",
      "Iteration 60, loss = 1.52074090\n",
      "Iteration 61, loss = 1.52023999\n",
      "Iteration 62, loss = 1.51972779\n",
      "Iteration 63, loss = 1.51933579\n",
      "Iteration 64, loss = 1.51883545\n",
      "Iteration 65, loss = 1.51835124\n",
      "Iteration 66, loss = 1.51782766\n",
      "Iteration 67, loss = 1.51743479\n",
      "Iteration 68, loss = 1.51690373\n",
      "Iteration 69, loss = 1.51644457\n",
      "Iteration 70, loss = 1.51603466\n",
      "Iteration 71, loss = 1.51561507\n",
      "Iteration 72, loss = 1.51514909\n",
      "Iteration 73, loss = 1.51471937\n",
      "Iteration 74, loss = 1.51419948\n",
      "Iteration 75, loss = 1.51381094\n",
      "Iteration 76, loss = 1.51337831\n",
      "Iteration 77, loss = 1.51296371\n",
      "Iteration 78, loss = 1.51252922\n",
      "Iteration 79, loss = 1.51212385\n",
      "Iteration 80, loss = 1.51161864\n",
      "Iteration 81, loss = 1.51122771\n",
      "Iteration 82, loss = 1.51087678\n",
      "Iteration 83, loss = 1.51045469\n",
      "Iteration 84, loss = 1.51001260\n",
      "Iteration 85, loss = 1.50954112\n",
      "Iteration 86, loss = 1.50915635\n",
      "Iteration 87, loss = 1.50879005\n",
      "Iteration 88, loss = 1.50839847\n",
      "Iteration 89, loss = 1.50806363\n",
      "Iteration 90, loss = 1.50757160\n",
      "Iteration 91, loss = 1.50720929\n",
      "Iteration 92, loss = 1.50684727\n",
      "Iteration 93, loss = 1.50641200\n",
      "Iteration 94, loss = 1.50602272\n",
      "Iteration 95, loss = 1.50566353\n",
      "Iteration 96, loss = 1.50531656\n",
      "Iteration 97, loss = 1.50486018\n",
      "Iteration 98, loss = 1.50452029\n",
      "Iteration 99, loss = 1.50415319\n",
      "Iteration 100, loss = 1.50368902\n",
      "Iteration 101, loss = 1.50340096\n",
      "Iteration 102, loss = 1.50294549\n",
      "Iteration 103, loss = 1.50264547\n",
      "Iteration 104, loss = 1.50218270\n",
      "Iteration 105, loss = 1.50176040\n",
      "Iteration 106, loss = 1.50147339\n",
      "Iteration 107, loss = 1.50106555\n",
      "Iteration 108, loss = 1.50077521\n",
      "Iteration 109, loss = 1.50034640\n",
      "Iteration 110, loss = 1.49994251\n",
      "Iteration 111, loss = 1.49965838\n",
      "Iteration 112, loss = 1.49928633\n",
      "Iteration 113, loss = 1.49886271\n",
      "Iteration 114, loss = 1.49859341\n",
      "Iteration 115, loss = 1.49813780\n",
      "Iteration 116, loss = 1.49787028\n",
      "Iteration 117, loss = 1.49747517\n",
      "Iteration 118, loss = 1.49718891\n",
      "Iteration 119, loss = 1.49672670\n",
      "Iteration 120, loss = 1.49631425\n",
      "Iteration 121, loss = 1.49603386\n",
      "Iteration 122, loss = 1.49552919\n",
      "Iteration 123, loss = 1.49524187\n",
      "Iteration 124, loss = 1.49495159\n",
      "Iteration 125, loss = 1.49452222\n",
      "Iteration 126, loss = 1.49417239\n",
      "Iteration 127, loss = 1.49390837\n",
      "Iteration 128, loss = 1.49346189\n",
      "Iteration 129, loss = 1.49319247\n",
      "Iteration 130, loss = 1.49290556\n",
      "Iteration 131, loss = 1.49244886\n",
      "Iteration 132, loss = 1.49224054\n",
      "Iteration 133, loss = 1.49173349\n",
      "Iteration 134, loss = 1.49148499\n",
      "Iteration 135, loss = 1.49116438\n",
      "Iteration 136, loss = 1.49068377\n",
      "Iteration 137, loss = 1.49045862\n",
      "Iteration 138, loss = 1.49002580\n",
      "Iteration 139, loss = 1.48965539\n",
      "Iteration 140, loss = 1.48935187\n",
      "Iteration 141, loss = 1.48906666\n",
      "Iteration 142, loss = 1.48873551\n",
      "Iteration 143, loss = 1.48835641\n",
      "Iteration 144, loss = 1.48802642\n",
      "Iteration 145, loss = 1.48760893\n",
      "Iteration 146, loss = 1.48737062\n",
      "Iteration 147, loss = 1.48695473\n",
      "Iteration 148, loss = 1.48676167\n",
      "Iteration 149, loss = 1.48632098\n",
      "Iteration 150, loss = 1.48590503\n",
      "Iteration 151, loss = 1.48558163\n",
      "Iteration 152, loss = 1.48522898\n",
      "Iteration 153, loss = 1.48496419\n",
      "Iteration 154, loss = 1.48455234\n",
      "Iteration 155, loss = 1.48425870\n",
      "Iteration 156, loss = 1.48387978\n",
      "Iteration 157, loss = 1.48369390\n",
      "Iteration 158, loss = 1.48323197\n",
      "Iteration 159, loss = 1.48294896\n",
      "Iteration 160, loss = 1.48261564\n",
      "Iteration 161, loss = 1.48228712\n",
      "Iteration 162, loss = 1.48197752\n",
      "Iteration 163, loss = 1.48172023\n",
      "Iteration 164, loss = 1.48124320\n",
      "Iteration 165, loss = 1.48083297\n",
      "Iteration 166, loss = 1.48052923\n",
      "Iteration 167, loss = 1.48030897\n",
      "Iteration 168, loss = 1.47994157\n",
      "Iteration 169, loss = 1.47960567\n",
      "Iteration 170, loss = 1.47921733\n",
      "Iteration 171, loss = 1.47893679\n",
      "Iteration 172, loss = 1.47876440\n",
      "Iteration 173, loss = 1.47831020\n",
      "Iteration 174, loss = 1.47797034\n",
      "Iteration 175, loss = 1.47768165\n",
      "Iteration 176, loss = 1.47745526\n",
      "Iteration 177, loss = 1.47707147\n",
      "Iteration 178, loss = 1.47661842\n",
      "Iteration 179, loss = 1.47646642\n",
      "Iteration 180, loss = 1.47606880\n",
      "Iteration 181, loss = 1.47559152\n",
      "Iteration 182, loss = 1.47540380\n",
      "Iteration 183, loss = 1.47491286\n",
      "Iteration 184, loss = 1.47475938\n",
      "Iteration 185, loss = 1.47455476\n",
      "Iteration 186, loss = 1.47406874\n",
      "Iteration 187, loss = 1.47382012\n",
      "Iteration 188, loss = 1.47336698\n",
      "Iteration 189, loss = 1.47322538\n",
      "Iteration 190, loss = 1.47297359\n",
      "Iteration 191, loss = 1.47245134\n",
      "Iteration 192, loss = 1.47235449\n",
      "Iteration 193, loss = 1.47194608\n",
      "Iteration 194, loss = 1.47170182\n",
      "Iteration 195, loss = 1.47140892\n",
      "Iteration 196, loss = 1.47103058\n",
      "Iteration 197, loss = 1.47074177\n",
      "Iteration 198, loss = 1.47053311\n",
      "Iteration 199, loss = 1.47025259\n",
      "Iteration 200, loss = 1.46964163\n",
      "Iteration 201, loss = 1.46968712\n",
      "Iteration 202, loss = 1.46917035\n",
      "Iteration 203, loss = 1.46887887\n",
      "Iteration 204, loss = 1.46885179\n",
      "Iteration 205, loss = 1.46845860\n",
      "Iteration 206, loss = 1.46798369\n",
      "Iteration 207, loss = 1.46788368\n",
      "Iteration 208, loss = 1.46755392\n",
      "Iteration 209, loss = 1.46709783\n",
      "Iteration 210, loss = 1.46698663\n",
      "Iteration 211, loss = 1.46652724\n",
      "Iteration 212, loss = 1.46633288\n",
      "Iteration 213, loss = 1.46596077\n",
      "Iteration 214, loss = 1.46558042\n",
      "Iteration 215, loss = 1.46549584\n",
      "Iteration 216, loss = 1.46499326\n",
      "Iteration 217, loss = 1.46499442\n",
      "Iteration 218, loss = 1.46461171\n",
      "Iteration 219, loss = 1.46441617\n",
      "Iteration 220, loss = 1.46417360\n",
      "Iteration 221, loss = 1.46372252\n",
      "Iteration 222, loss = 1.46348010\n",
      "Iteration 223, loss = 1.46313840\n",
      "Iteration 224, loss = 1.46319014\n",
      "Iteration 225, loss = 1.46269718\n",
      "Iteration 226, loss = 1.46244701\n",
      "Iteration 227, loss = 1.46211820\n",
      "Iteration 228, loss = 1.46180890\n",
      "Iteration 229, loss = 1.46156748\n",
      "Iteration 230, loss = 1.46151981\n",
      "Iteration 231, loss = 1.46119365\n",
      "Iteration 232, loss = 1.46078217\n",
      "Iteration 233, loss = 1.46075066\n",
      "Iteration 234, loss = 1.46023695\n",
      "Iteration 235, loss = 1.46009598\n",
      "Iteration 236, loss = 1.45971632\n",
      "Iteration 237, loss = 1.45940300\n",
      "Iteration 238, loss = 1.45939848\n",
      "Iteration 239, loss = 1.45886175\n",
      "Iteration 240, loss = 1.45890455\n",
      "Iteration 241, loss = 1.45866027\n",
      "Iteration 242, loss = 1.45844817\n",
      "Iteration 243, loss = 1.45803624\n",
      "Iteration 244, loss = 1.45775260\n",
      "Iteration 245, loss = 1.45757728\n",
      "Iteration 246, loss = 1.45720499\n",
      "Iteration 247, loss = 1.45711142\n",
      "Iteration 248, loss = 1.45701635\n",
      "Iteration 249, loss = 1.45653717\n",
      "Iteration 250, loss = 1.45627870\n",
      "Iteration 251, loss = 1.45628913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 252, loss = 1.45568955\n",
      "Iteration 253, loss = 1.45560485\n",
      "Iteration 254, loss = 1.45551097\n",
      "Iteration 255, loss = 1.45523506\n",
      "Iteration 256, loss = 1.45501079\n",
      "Iteration 257, loss = 1.45459082\n",
      "Iteration 258, loss = 1.45446983\n",
      "Iteration 259, loss = 1.45422648\n",
      "Iteration 260, loss = 1.45396024\n",
      "Iteration 261, loss = 1.45385896\n",
      "Iteration 262, loss = 1.45337798\n",
      "Iteration 263, loss = 1.45337996\n",
      "Iteration 264, loss = 1.45331835\n",
      "Iteration 265, loss = 1.45296310\n",
      "Iteration 266, loss = 1.45262614\n",
      "Iteration 267, loss = 1.45263699\n",
      "Iteration 268, loss = 1.45235810\n",
      "Iteration 269, loss = 1.45208021\n",
      "Iteration 270, loss = 1.45166553\n",
      "Iteration 271, loss = 1.45162268\n",
      "Iteration 272, loss = 1.45138862\n",
      "Iteration 273, loss = 1.45114667\n",
      "Iteration 274, loss = 1.45076530\n",
      "Iteration 275, loss = 1.45097491\n",
      "Iteration 276, loss = 1.45046327\n",
      "Iteration 277, loss = 1.45028129\n",
      "Iteration 278, loss = 1.45017315\n",
      "Iteration 279, loss = 1.44978520\n",
      "Iteration 280, loss = 1.44987111\n",
      "Iteration 281, loss = 1.44959066\n",
      "Iteration 282, loss = 1.44931318\n",
      "Iteration 283, loss = 1.44953503\n",
      "Iteration 284, loss = 1.44889170\n",
      "Iteration 285, loss = 1.44875242\n",
      "Iteration 286, loss = 1.44851820\n",
      "Iteration 287, loss = 1.44829678\n",
      "Iteration 288, loss = 1.44830559\n",
      "Iteration 289, loss = 1.44795502\n",
      "Iteration 290, loss = 1.44776093\n",
      "Iteration 291, loss = 1.44752774\n",
      "Iteration 292, loss = 1.44750808\n",
      "Iteration 293, loss = 1.44735648\n",
      "Iteration 294, loss = 1.44714504\n",
      "Iteration 295, loss = 1.44681371\n",
      "Iteration 296, loss = 1.44658790\n",
      "Iteration 297, loss = 1.44623686\n",
      "Iteration 298, loss = 1.44603891\n",
      "Iteration 299, loss = 1.44597111\n",
      "Iteration 300, loss = 1.44593716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end:  3\n",
      "iteration:  4\n",
      "fit\n",
      "Iteration 1, loss = 1.64100277\n",
      "Iteration 2, loss = 1.59281486\n",
      "Iteration 3, loss = 1.58003972\n",
      "Iteration 4, loss = 1.57285824\n",
      "Iteration 5, loss = 1.56788184\n",
      "Iteration 6, loss = 1.56405900\n",
      "Iteration 7, loss = 1.56111205\n",
      "Iteration 8, loss = 1.55859105\n",
      "Iteration 9, loss = 1.55658671\n",
      "Iteration 10, loss = 1.55485576\n",
      "Iteration 11, loss = 1.55322079\n",
      "Iteration 12, loss = 1.55185391\n",
      "Iteration 13, loss = 1.55060653\n",
      "Iteration 14, loss = 1.54942786\n",
      "Iteration 15, loss = 1.54838278\n",
      "Iteration 16, loss = 1.54733298\n",
      "Iteration 17, loss = 1.54638760\n",
      "Iteration 18, loss = 1.54540633\n",
      "Iteration 19, loss = 1.54452349\n",
      "Iteration 20, loss = 1.54366061\n",
      "Iteration 21, loss = 1.54277445\n",
      "Iteration 22, loss = 1.54197946\n",
      "Iteration 23, loss = 1.54118366\n",
      "Iteration 24, loss = 1.54038606\n",
      "Iteration 25, loss = 1.53961432\n",
      "Iteration 26, loss = 1.53889762\n",
      "Iteration 27, loss = 1.53822095\n",
      "Iteration 28, loss = 1.53747247\n",
      "Iteration 29, loss = 1.53680359\n",
      "Iteration 30, loss = 1.53609372\n",
      "Iteration 31, loss = 1.53545037\n",
      "Iteration 32, loss = 1.53488485\n",
      "Iteration 33, loss = 1.53425282\n",
      "Iteration 34, loss = 1.53362330\n",
      "Iteration 35, loss = 1.53296495\n",
      "Iteration 36, loss = 1.53246776\n",
      "Iteration 37, loss = 1.53189091\n",
      "Iteration 38, loss = 1.53126975\n",
      "Iteration 39, loss = 1.53075932\n",
      "Iteration 40, loss = 1.53016778\n",
      "Iteration 41, loss = 1.52959461\n",
      "Iteration 42, loss = 1.52908095\n",
      "Iteration 43, loss = 1.52850468\n",
      "Iteration 44, loss = 1.52797372\n",
      "Iteration 45, loss = 1.52749106\n",
      "Iteration 46, loss = 1.52700331\n",
      "Iteration 47, loss = 1.52645198\n",
      "Iteration 48, loss = 1.52592518\n",
      "Iteration 49, loss = 1.52544097\n",
      "Iteration 50, loss = 1.52489949\n",
      "Iteration 51, loss = 1.52435755\n",
      "Iteration 52, loss = 1.52397100\n",
      "Iteration 53, loss = 1.52347089\n",
      "Iteration 54, loss = 1.52294584\n",
      "Iteration 55, loss = 1.52247748\n",
      "Iteration 56, loss = 1.52197057\n",
      "Iteration 57, loss = 1.52153113\n",
      "Iteration 58, loss = 1.52096146\n",
      "Iteration 59, loss = 1.52063433\n",
      "Iteration 60, loss = 1.52009452\n",
      "Iteration 61, loss = 1.51967347\n",
      "Iteration 62, loss = 1.51919961\n",
      "Iteration 63, loss = 1.51878182\n",
      "Iteration 64, loss = 1.51833514\n",
      "Iteration 65, loss = 1.51788679\n",
      "Iteration 66, loss = 1.51739231\n",
      "Iteration 67, loss = 1.51701974\n",
      "Iteration 68, loss = 1.51654343\n",
      "Iteration 69, loss = 1.51608136\n",
      "Iteration 70, loss = 1.51569451\n",
      "Iteration 71, loss = 1.51525064\n",
      "Iteration 72, loss = 1.51481867\n",
      "Iteration 73, loss = 1.51449691\n",
      "Iteration 74, loss = 1.51400387\n",
      "Iteration 75, loss = 1.51359229\n",
      "Iteration 76, loss = 1.51316823\n",
      "Iteration 77, loss = 1.51280992\n",
      "Iteration 78, loss = 1.51236818\n",
      "Iteration 79, loss = 1.51199935\n",
      "Iteration 80, loss = 1.51151408\n",
      "Iteration 81, loss = 1.51120818\n",
      "Iteration 82, loss = 1.51084667\n",
      "Iteration 83, loss = 1.51043805\n",
      "Iteration 84, loss = 1.51003125\n",
      "Iteration 85, loss = 1.50958072\n",
      "Iteration 86, loss = 1.50919811\n",
      "Iteration 87, loss = 1.50886357\n",
      "Iteration 88, loss = 1.50844296\n",
      "Iteration 89, loss = 1.50815766\n",
      "Iteration 90, loss = 1.50773567\n",
      "Iteration 91, loss = 1.50731065\n",
      "Iteration 92, loss = 1.50696600\n",
      "Iteration 93, loss = 1.50655529\n",
      "Iteration 94, loss = 1.50627222\n",
      "Iteration 95, loss = 1.50591921\n",
      "Iteration 96, loss = 1.50543067\n",
      "Iteration 97, loss = 1.50515455\n",
      "Iteration 98, loss = 1.50482311\n",
      "Iteration 99, loss = 1.50433096\n",
      "Iteration 100, loss = 1.50398266\n",
      "Iteration 101, loss = 1.50364076\n",
      "Iteration 102, loss = 1.50325792\n",
      "Iteration 103, loss = 1.50296825\n",
      "Iteration 104, loss = 1.50250740\n",
      "Iteration 105, loss = 1.50219472\n",
      "Iteration 106, loss = 1.50186001\n",
      "Iteration 107, loss = 1.50144681\n",
      "Iteration 108, loss = 1.50118728\n",
      "Iteration 109, loss = 1.50078731\n",
      "Iteration 110, loss = 1.50043521\n",
      "Iteration 111, loss = 1.50009435\n",
      "Iteration 112, loss = 1.49979186\n",
      "Iteration 113, loss = 1.49937210\n",
      "Iteration 114, loss = 1.49908923\n",
      "Iteration 115, loss = 1.49869091\n",
      "Iteration 116, loss = 1.49833968\n",
      "Iteration 117, loss = 1.49795909\n",
      "Iteration 118, loss = 1.49770649\n",
      "Iteration 119, loss = 1.49720930\n",
      "Iteration 120, loss = 1.49690451\n",
      "Iteration 121, loss = 1.49651008\n",
      "Iteration 122, loss = 1.49615275\n",
      "Iteration 123, loss = 1.49591098\n",
      "Iteration 124, loss = 1.49548826\n",
      "Iteration 125, loss = 1.49515426\n",
      "Iteration 126, loss = 1.49484850\n",
      "Iteration 127, loss = 1.49455696\n",
      "Iteration 128, loss = 1.49411251\n",
      "Iteration 129, loss = 1.49391836\n",
      "Iteration 130, loss = 1.49352067\n",
      "Iteration 131, loss = 1.49310595\n",
      "Iteration 132, loss = 1.49274590\n",
      "Iteration 133, loss = 1.49240103\n",
      "Iteration 134, loss = 1.49203243\n",
      "Iteration 135, loss = 1.49176794\n",
      "Iteration 136, loss = 1.49139284\n",
      "Iteration 137, loss = 1.49107768\n",
      "Iteration 138, loss = 1.49073085\n",
      "Iteration 139, loss = 1.49039518\n",
      "Iteration 140, loss = 1.48999879\n",
      "Iteration 141, loss = 1.48979079\n",
      "Iteration 142, loss = 1.48935822\n",
      "Iteration 143, loss = 1.48912382\n",
      "Iteration 144, loss = 1.48884611\n",
      "Iteration 145, loss = 1.48835663\n",
      "Iteration 146, loss = 1.48805150\n",
      "Iteration 147, loss = 1.48769036\n",
      "Iteration 148, loss = 1.48753954\n",
      "Iteration 149, loss = 1.48713515\n",
      "Iteration 150, loss = 1.48682259\n",
      "Iteration 151, loss = 1.48639276\n",
      "Iteration 152, loss = 1.48601104\n",
      "Iteration 153, loss = 1.48577704\n",
      "Iteration 154, loss = 1.48541372\n",
      "Iteration 155, loss = 1.48518891\n",
      "Iteration 156, loss = 1.48479580\n",
      "Iteration 157, loss = 1.48448685\n",
      "Iteration 158, loss = 1.48407311\n",
      "Iteration 159, loss = 1.48385217\n",
      "Iteration 160, loss = 1.48350642\n",
      "Iteration 161, loss = 1.48316032\n",
      "Iteration 162, loss = 1.48290167\n",
      "Iteration 163, loss = 1.48262093\n",
      "Iteration 164, loss = 1.48224536\n",
      "Iteration 165, loss = 1.48183537\n",
      "Iteration 166, loss = 1.48148192\n",
      "Iteration 167, loss = 1.48135580\n",
      "Iteration 168, loss = 1.48089222\n",
      "Iteration 169, loss = 1.48049436\n",
      "Iteration 170, loss = 1.48029662\n",
      "Iteration 171, loss = 1.48006197\n",
      "Iteration 172, loss = 1.47974796\n",
      "Iteration 173, loss = 1.47948207\n",
      "Iteration 174, loss = 1.47909200\n",
      "Iteration 175, loss = 1.47883990\n",
      "Iteration 176, loss = 1.47842479\n",
      "Iteration 177, loss = 1.47802566\n",
      "Iteration 178, loss = 1.47777618\n",
      "Iteration 179, loss = 1.47754182\n",
      "Iteration 180, loss = 1.47711600\n",
      "Iteration 181, loss = 1.47688127\n",
      "Iteration 182, loss = 1.47653042\n",
      "Iteration 183, loss = 1.47608746\n",
      "Iteration 184, loss = 1.47593464\n",
      "Iteration 185, loss = 1.47569550\n",
      "Iteration 186, loss = 1.47537673\n",
      "Iteration 187, loss = 1.47505801\n",
      "Iteration 188, loss = 1.47462908\n",
      "Iteration 189, loss = 1.47439417\n",
      "Iteration 190, loss = 1.47412627\n",
      "Iteration 191, loss = 1.47378593\n",
      "Iteration 192, loss = 1.47365951\n",
      "Iteration 193, loss = 1.47322325\n",
      "Iteration 194, loss = 1.47297125\n",
      "Iteration 195, loss = 1.47272467\n",
      "Iteration 196, loss = 1.47232083\n",
      "Iteration 197, loss = 1.47213662\n",
      "Iteration 198, loss = 1.47175643\n",
      "Iteration 199, loss = 1.47154056\n",
      "Iteration 200, loss = 1.47108113\n",
      "Iteration 201, loss = 1.47084760\n",
      "Iteration 202, loss = 1.47066444\n",
      "Iteration 203, loss = 1.47032850\n",
      "Iteration 204, loss = 1.46997224\n",
      "Iteration 205, loss = 1.46986363\n",
      "Iteration 206, loss = 1.46943753\n",
      "Iteration 207, loss = 1.46924443\n",
      "Iteration 208, loss = 1.46880793\n",
      "Iteration 209, loss = 1.46852636\n",
      "Iteration 210, loss = 1.46839921\n",
      "Iteration 211, loss = 1.46793807\n",
      "Iteration 212, loss = 1.46766999\n",
      "Iteration 213, loss = 1.46739073\n",
      "Iteration 214, loss = 1.46694181\n",
      "Iteration 215, loss = 1.46679501\n",
      "Iteration 216, loss = 1.46655630\n",
      "Iteration 217, loss = 1.46632607\n",
      "Iteration 218, loss = 1.46618910\n",
      "Iteration 219, loss = 1.46588780\n",
      "Iteration 220, loss = 1.46552767\n",
      "Iteration 221, loss = 1.46508016\n",
      "Iteration 222, loss = 1.46493911\n",
      "Iteration 223, loss = 1.46472044\n",
      "Iteration 224, loss = 1.46452196\n",
      "Iteration 225, loss = 1.46430071\n",
      "Iteration 226, loss = 1.46382896\n",
      "Iteration 227, loss = 1.46361282\n",
      "Iteration 228, loss = 1.46338242\n",
      "Iteration 229, loss = 1.46305930\n",
      "Iteration 230, loss = 1.46301214\n",
      "Iteration 231, loss = 1.46249655\n",
      "Iteration 232, loss = 1.46236914\n",
      "Iteration 233, loss = 1.46223061\n",
      "Iteration 234, loss = 1.46179272\n",
      "Iteration 235, loss = 1.46168347\n",
      "Iteration 236, loss = 1.46138740\n",
      "Iteration 237, loss = 1.46093440\n",
      "Iteration 238, loss = 1.46093398\n",
      "Iteration 239, loss = 1.46041636\n",
      "Iteration 240, loss = 1.46032202\n",
      "Iteration 241, loss = 1.46002492\n",
      "Iteration 242, loss = 1.45983045\n",
      "Iteration 243, loss = 1.45960506\n",
      "Iteration 244, loss = 1.45915111\n",
      "Iteration 245, loss = 1.45906664\n",
      "Iteration 246, loss = 1.45863336\n",
      "Iteration 247, loss = 1.45863329\n",
      "Iteration 248, loss = 1.45851004\n",
      "Iteration 249, loss = 1.45797353\n",
      "Iteration 250, loss = 1.45766368\n",
      "Iteration 251, loss = 1.45770234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 252, loss = 1.45724308\n",
      "Iteration 253, loss = 1.45710074\n",
      "Iteration 254, loss = 1.45698473\n",
      "Iteration 255, loss = 1.45659998\n",
      "Iteration 256, loss = 1.45633333\n",
      "Iteration 257, loss = 1.45601824\n",
      "Iteration 258, loss = 1.45579890\n",
      "Iteration 259, loss = 1.45552779\n",
      "Iteration 260, loss = 1.45550286\n",
      "Iteration 261, loss = 1.45512420\n",
      "Iteration 262, loss = 1.45505257\n",
      "Iteration 263, loss = 1.45472535\n",
      "Iteration 264, loss = 1.45462081\n",
      "Iteration 265, loss = 1.45418985\n",
      "Iteration 266, loss = 1.45420066\n",
      "Iteration 267, loss = 1.45401572\n",
      "Iteration 268, loss = 1.45366892\n",
      "Iteration 269, loss = 1.45342674\n",
      "Iteration 270, loss = 1.45320873\n",
      "Iteration 271, loss = 1.45314053\n",
      "Iteration 272, loss = 1.45284292\n",
      "Iteration 273, loss = 1.45266062\n",
      "Iteration 274, loss = 1.45226552\n",
      "Iteration 275, loss = 1.45198036\n",
      "Iteration 276, loss = 1.45186383\n",
      "Iteration 277, loss = 1.45177899\n",
      "Iteration 278, loss = 1.45156804\n",
      "Iteration 279, loss = 1.45114803\n",
      "Iteration 280, loss = 1.45099002\n",
      "Iteration 281, loss = 1.45084579\n",
      "Iteration 282, loss = 1.45051763\n",
      "Iteration 283, loss = 1.45056998\n",
      "Iteration 284, loss = 1.45022886\n",
      "Iteration 285, loss = 1.45002865\n",
      "Iteration 286, loss = 1.44948857\n",
      "Iteration 287, loss = 1.44958362\n",
      "Iteration 288, loss = 1.44950851\n",
      "Iteration 289, loss = 1.44917520\n",
      "Iteration 290, loss = 1.44904327\n",
      "Iteration 291, loss = 1.44868980\n",
      "Iteration 292, loss = 1.44857327\n",
      "Iteration 293, loss = 1.44845654\n",
      "Iteration 294, loss = 1.44813974\n",
      "Iteration 295, loss = 1.44801002\n",
      "Iteration 296, loss = 1.44766143\n",
      "Iteration 297, loss = 1.44737065\n",
      "Iteration 298, loss = 1.44735857\n",
      "Iteration 299, loss = 1.44714452\n",
      "Iteration 300, loss = 1.44695245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end:  4\n",
      "iteration:  5\n",
      "fit\n",
      "Iteration 1, loss = 1.64126139\n",
      "Iteration 2, loss = 1.59313253\n",
      "Iteration 3, loss = 1.58042438\n",
      "Iteration 4, loss = 1.57328303\n",
      "Iteration 5, loss = 1.56836016\n",
      "Iteration 6, loss = 1.56457194\n",
      "Iteration 7, loss = 1.56165335\n",
      "Iteration 8, loss = 1.55919055\n",
      "Iteration 9, loss = 1.55715673\n",
      "Iteration 10, loss = 1.55538644\n",
      "Iteration 11, loss = 1.55375305\n",
      "Iteration 12, loss = 1.55240151\n",
      "Iteration 13, loss = 1.55114410\n",
      "Iteration 14, loss = 1.54996167\n",
      "Iteration 15, loss = 1.54888504\n",
      "Iteration 16, loss = 1.54787339\n",
      "Iteration 17, loss = 1.54689088\n",
      "Iteration 18, loss = 1.54594920\n",
      "Iteration 19, loss = 1.54501609\n",
      "Iteration 20, loss = 1.54412434\n",
      "Iteration 21, loss = 1.54326969\n",
      "Iteration 22, loss = 1.54244818\n",
      "Iteration 23, loss = 1.54162092\n",
      "Iteration 24, loss = 1.54086175\n",
      "Iteration 25, loss = 1.54004863\n",
      "Iteration 26, loss = 1.53935175\n",
      "Iteration 27, loss = 1.53860796\n",
      "Iteration 28, loss = 1.53787618\n",
      "Iteration 29, loss = 1.53718090\n",
      "Iteration 30, loss = 1.53641960\n",
      "Iteration 31, loss = 1.53582822\n",
      "Iteration 32, loss = 1.53522407\n",
      "Iteration 33, loss = 1.53458406\n",
      "Iteration 34, loss = 1.53397042\n",
      "Iteration 35, loss = 1.53335774\n",
      "Iteration 36, loss = 1.53276010\n",
      "Iteration 37, loss = 1.53222033\n",
      "Iteration 38, loss = 1.53158784\n",
      "Iteration 39, loss = 1.53097237\n",
      "Iteration 40, loss = 1.53042727\n",
      "Iteration 41, loss = 1.52991471\n",
      "Iteration 42, loss = 1.52937691\n",
      "Iteration 43, loss = 1.52880168\n",
      "Iteration 44, loss = 1.52818517\n",
      "Iteration 45, loss = 1.52770807\n",
      "Iteration 46, loss = 1.52720270\n",
      "Iteration 47, loss = 1.52664821\n",
      "Iteration 48, loss = 1.52615877\n",
      "Iteration 49, loss = 1.52562839\n",
      "Iteration 50, loss = 1.52514559\n",
      "Iteration 51, loss = 1.52458043\n",
      "Iteration 52, loss = 1.52413430\n",
      "Iteration 53, loss = 1.52367562\n",
      "Iteration 54, loss = 1.52314117\n",
      "Iteration 55, loss = 1.52268274\n",
      "Iteration 56, loss = 1.52219826\n",
      "Iteration 57, loss = 1.52167432\n",
      "Iteration 58, loss = 1.52114367\n",
      "Iteration 59, loss = 1.52079250\n",
      "Iteration 60, loss = 1.52027454\n",
      "Iteration 61, loss = 1.51980564\n",
      "Iteration 62, loss = 1.51939247\n",
      "Iteration 63, loss = 1.51896607\n",
      "Iteration 64, loss = 1.51843785\n",
      "Iteration 65, loss = 1.51802522\n",
      "Iteration 66, loss = 1.51757437\n",
      "Iteration 67, loss = 1.51715921\n",
      "Iteration 68, loss = 1.51667681\n",
      "Iteration 69, loss = 1.51621026\n",
      "Iteration 70, loss = 1.51580138\n",
      "Iteration 71, loss = 1.51536976\n",
      "Iteration 72, loss = 1.51488855\n",
      "Iteration 73, loss = 1.51454782\n",
      "Iteration 74, loss = 1.51409551\n",
      "Iteration 75, loss = 1.51360819\n",
      "Iteration 76, loss = 1.51325632\n",
      "Iteration 77, loss = 1.51285802\n",
      "Iteration 78, loss = 1.51241666\n",
      "Iteration 79, loss = 1.51203142\n",
      "Iteration 80, loss = 1.51155338\n",
      "Iteration 81, loss = 1.51123259\n",
      "Iteration 82, loss = 1.51084205\n",
      "Iteration 83, loss = 1.51042020\n",
      "Iteration 84, loss = 1.51000655\n",
      "Iteration 85, loss = 1.50957087\n",
      "Iteration 86, loss = 1.50918913\n",
      "Iteration 87, loss = 1.50883638\n",
      "Iteration 88, loss = 1.50838463\n",
      "Iteration 89, loss = 1.50807088\n",
      "Iteration 90, loss = 1.50762535\n",
      "Iteration 91, loss = 1.50719145\n",
      "Iteration 92, loss = 1.50684381\n",
      "Iteration 93, loss = 1.50641987\n",
      "Iteration 94, loss = 1.50607938\n",
      "Iteration 95, loss = 1.50564398\n",
      "Iteration 96, loss = 1.50526348\n",
      "Iteration 97, loss = 1.50494098\n",
      "Iteration 98, loss = 1.50451823\n",
      "Iteration 99, loss = 1.50416473\n",
      "Iteration 100, loss = 1.50377519\n",
      "Iteration 101, loss = 1.50344655\n",
      "Iteration 102, loss = 1.50301929\n",
      "Iteration 103, loss = 1.50273289\n",
      "Iteration 104, loss = 1.50220457\n",
      "Iteration 105, loss = 1.50194902\n",
      "Iteration 106, loss = 1.50159183\n",
      "Iteration 107, loss = 1.50121025\n",
      "Iteration 108, loss = 1.50086863\n",
      "Iteration 109, loss = 1.50043750\n",
      "Iteration 110, loss = 1.50011064\n",
      "Iteration 111, loss = 1.49981519\n",
      "Iteration 112, loss = 1.49942100\n",
      "Iteration 113, loss = 1.49905322\n",
      "Iteration 114, loss = 1.49872504\n",
      "Iteration 115, loss = 1.49834844\n",
      "Iteration 116, loss = 1.49794266\n",
      "Iteration 117, loss = 1.49756836\n",
      "Iteration 118, loss = 1.49733735\n",
      "Iteration 119, loss = 1.49689513\n",
      "Iteration 120, loss = 1.49650392\n",
      "Iteration 121, loss = 1.49622254\n",
      "Iteration 122, loss = 1.49576114\n",
      "Iteration 123, loss = 1.49554267\n",
      "Iteration 124, loss = 1.49514731\n",
      "Iteration 125, loss = 1.49482054\n",
      "Iteration 126, loss = 1.49450284\n",
      "Iteration 127, loss = 1.49423046\n",
      "Iteration 128, loss = 1.49376065\n",
      "Iteration 129, loss = 1.49348532\n",
      "Iteration 130, loss = 1.49306364\n",
      "Iteration 131, loss = 1.49281082\n",
      "Iteration 132, loss = 1.49241796\n",
      "Iteration 133, loss = 1.49210095\n",
      "Iteration 134, loss = 1.49177089\n",
      "Iteration 135, loss = 1.49140547\n",
      "Iteration 136, loss = 1.49105843\n",
      "Iteration 137, loss = 1.49075176\n",
      "Iteration 138, loss = 1.49047574\n",
      "Iteration 139, loss = 1.49011155\n",
      "Iteration 140, loss = 1.48975999\n",
      "Iteration 141, loss = 1.48945599\n",
      "Iteration 142, loss = 1.48906754\n",
      "Iteration 143, loss = 1.48888087\n",
      "Iteration 144, loss = 1.48849782\n",
      "Iteration 145, loss = 1.48809512\n",
      "Iteration 146, loss = 1.48776471\n",
      "Iteration 147, loss = 1.48740678\n",
      "Iteration 148, loss = 1.48720522\n",
      "Iteration 149, loss = 1.48680386\n",
      "Iteration 150, loss = 1.48652197\n",
      "Iteration 151, loss = 1.48617396\n",
      "Iteration 152, loss = 1.48576560\n",
      "Iteration 153, loss = 1.48547047\n",
      "Iteration 154, loss = 1.48509901\n",
      "Iteration 155, loss = 1.48489887\n",
      "Iteration 156, loss = 1.48450979\n",
      "Iteration 157, loss = 1.48428710\n",
      "Iteration 158, loss = 1.48380668\n",
      "Iteration 159, loss = 1.48349432\n",
      "Iteration 160, loss = 1.48320889\n",
      "Iteration 161, loss = 1.48284263\n",
      "Iteration 162, loss = 1.48266007\n",
      "Iteration 163, loss = 1.48230958\n",
      "Iteration 164, loss = 1.48193272\n",
      "Iteration 165, loss = 1.48145445\n",
      "Iteration 166, loss = 1.48123773\n",
      "Iteration 167, loss = 1.48094994\n",
      "Iteration 168, loss = 1.48070349\n",
      "Iteration 169, loss = 1.48035799\n",
      "Iteration 170, loss = 1.47995032\n",
      "Iteration 171, loss = 1.47978367\n",
      "Iteration 172, loss = 1.47946876\n",
      "Iteration 173, loss = 1.47915690\n",
      "Iteration 174, loss = 1.47875330\n",
      "Iteration 175, loss = 1.47853761\n",
      "Iteration 176, loss = 1.47825734\n",
      "Iteration 177, loss = 1.47771746\n",
      "Iteration 178, loss = 1.47758479\n",
      "Iteration 179, loss = 1.47726178\n",
      "Iteration 180, loss = 1.47695175\n",
      "Iteration 181, loss = 1.47661418\n",
      "Iteration 182, loss = 1.47635896\n",
      "Iteration 183, loss = 1.47591699\n",
      "Iteration 184, loss = 1.47562242\n",
      "Iteration 185, loss = 1.47542028\n",
      "Iteration 186, loss = 1.47519300\n",
      "Iteration 187, loss = 1.47496319\n",
      "Iteration 188, loss = 1.47444175\n",
      "Iteration 189, loss = 1.47429573\n",
      "Iteration 190, loss = 1.47391061\n",
      "Iteration 191, loss = 1.47360733\n",
      "Iteration 192, loss = 1.47323436\n",
      "Iteration 193, loss = 1.47303024\n",
      "Iteration 194, loss = 1.47268040\n",
      "Iteration 195, loss = 1.47256569\n",
      "Iteration 196, loss = 1.47219941\n",
      "Iteration 197, loss = 1.47197091\n",
      "Iteration 198, loss = 1.47161270\n",
      "Iteration 199, loss = 1.47133517\n",
      "Iteration 200, loss = 1.47102583\n",
      "Iteration 201, loss = 1.47085141\n",
      "Iteration 202, loss = 1.47046002\n",
      "Iteration 203, loss = 1.47016456\n",
      "Iteration 204, loss = 1.46996145\n",
      "Iteration 205, loss = 1.46977877\n",
      "Iteration 206, loss = 1.46934887\n",
      "Iteration 207, loss = 1.46911336\n",
      "Iteration 208, loss = 1.46873181\n",
      "Iteration 209, loss = 1.46849469\n",
      "Iteration 210, loss = 1.46835317\n",
      "Iteration 211, loss = 1.46791475\n",
      "Iteration 212, loss = 1.46775319\n",
      "Iteration 213, loss = 1.46742804\n",
      "Iteration 214, loss = 1.46688711\n",
      "Iteration 215, loss = 1.46681203\n",
      "Iteration 216, loss = 1.46657907\n",
      "Iteration 217, loss = 1.46629200\n",
      "Iteration 218, loss = 1.46610833\n",
      "Iteration 219, loss = 1.46592846\n",
      "Iteration 220, loss = 1.46559420\n",
      "Iteration 221, loss = 1.46511525\n",
      "Iteration 222, loss = 1.46485580\n",
      "Iteration 223, loss = 1.46462141\n",
      "Iteration 224, loss = 1.46464310\n",
      "Iteration 225, loss = 1.46420462\n",
      "Iteration 226, loss = 1.46377488\n",
      "Iteration 227, loss = 1.46375521\n",
      "Iteration 228, loss = 1.46359065\n",
      "Iteration 229, loss = 1.46306401\n",
      "Iteration 230, loss = 1.46300020\n",
      "Iteration 231, loss = 1.46270909\n",
      "Iteration 232, loss = 1.46257376\n",
      "Iteration 233, loss = 1.46216149\n",
      "Iteration 234, loss = 1.46200423\n",
      "Iteration 235, loss = 1.46170963\n",
      "Iteration 236, loss = 1.46149338\n",
      "Iteration 237, loss = 1.46109418\n",
      "Iteration 238, loss = 1.46089067\n",
      "Iteration 239, loss = 1.46048012\n",
      "Iteration 240, loss = 1.46039826\n",
      "Iteration 241, loss = 1.46019087\n",
      "Iteration 242, loss = 1.45999765\n",
      "Iteration 243, loss = 1.45964904\n",
      "Iteration 244, loss = 1.45937764\n",
      "Iteration 245, loss = 1.45918989\n",
      "Iteration 246, loss = 1.45881721\n",
      "Iteration 247, loss = 1.45865922\n",
      "Iteration 248, loss = 1.45864288\n",
      "Iteration 249, loss = 1.45809712\n",
      "Iteration 250, loss = 1.45795014\n",
      "Iteration 251, loss = 1.45794818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 252, loss = 1.45754847\n",
      "Iteration 253, loss = 1.45733203\n",
      "Iteration 254, loss = 1.45709956\n",
      "Iteration 255, loss = 1.45684546\n",
      "Iteration 256, loss = 1.45655452\n",
      "Iteration 257, loss = 1.45632717\n",
      "Iteration 258, loss = 1.45608772\n",
      "Iteration 259, loss = 1.45591813\n",
      "Iteration 260, loss = 1.45564854\n",
      "Iteration 261, loss = 1.45540918\n",
      "Iteration 262, loss = 1.45525120\n",
      "Iteration 263, loss = 1.45486942\n",
      "Iteration 264, loss = 1.45490735\n",
      "Iteration 265, loss = 1.45446599\n",
      "Iteration 266, loss = 1.45438971\n",
      "Iteration 267, loss = 1.45413442\n",
      "Iteration 268, loss = 1.45387117\n",
      "Iteration 269, loss = 1.45373237\n",
      "Iteration 270, loss = 1.45343954\n",
      "Iteration 271, loss = 1.45351310\n",
      "Iteration 272, loss = 1.45301997\n",
      "Iteration 273, loss = 1.45295992\n",
      "Iteration 274, loss = 1.45248052\n",
      "Iteration 275, loss = 1.45226060\n",
      "Iteration 276, loss = 1.45202963\n",
      "Iteration 277, loss = 1.45200040\n",
      "Iteration 278, loss = 1.45176462\n",
      "Iteration 279, loss = 1.45146938\n",
      "Iteration 280, loss = 1.45144799\n",
      "Iteration 281, loss = 1.45113859\n",
      "Iteration 282, loss = 1.45071039\n",
      "Iteration 283, loss = 1.45067160\n",
      "Iteration 284, loss = 1.45043968\n",
      "Iteration 285, loss = 1.45026382\n",
      "Iteration 286, loss = 1.44997748\n",
      "Iteration 287, loss = 1.44979387\n",
      "Iteration 288, loss = 1.44962267\n",
      "Iteration 289, loss = 1.44947078\n",
      "Iteration 290, loss = 1.44917408\n",
      "Iteration 291, loss = 1.44896609\n",
      "Iteration 292, loss = 1.44903896\n",
      "Iteration 293, loss = 1.44861886\n",
      "Iteration 294, loss = 1.44839591\n",
      "Iteration 295, loss = 1.44815660\n",
      "Iteration 296, loss = 1.44788976\n",
      "Iteration 297, loss = 1.44777984\n",
      "Iteration 298, loss = 1.44766948\n",
      "Iteration 299, loss = 1.44735489\n",
      "Iteration 300, loss = 1.44720208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end:  5\n"
     ]
    }
   ],
   "source": [
    "i = 1 \n",
    "reports = []\n",
    "for train_index, test_index in kf.split(inputs):\n",
    "    print('iteration: ', i)\n",
    "    #get data fold\n",
    "    X_train, X_test = inputs.iloc[train_index], inputs.iloc[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    \n",
    "    #start model \n",
    "    print('fit')\n",
    "    clf = MLPClassifier(random_state=0, \n",
    "                        max_iter=300,\n",
    "                       activation=params['activation'],\n",
    "                       alpha=params['alpha'],\n",
    "                       hidden_layer_sizes=params['hidden_layer_sizes'],\n",
    "                       learning_rate=params['learning_rate'],\n",
    "                       solver=params['solver'],\n",
    "                       verbose=5)\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    report = classification_report(y_test, \n",
    "                                   y_pred, \n",
    "                                   target_names=np.unique(metadataset['first_place'].values),\n",
    "                                  output_dict=True)\n",
    "    \n",
    "    bl_zeroes, bl_no_zeroes_most, bl_no_zeroes_best = base_level_eval(metadataset.iloc[test_index]['original_id'].values,\n",
    "             list(label_encoder.inverse_transform(y_pred)))\n",
    "\n",
    "    base_impact_with_zeroes.append(bl_zeroes)\n",
    "    base_impact_without_zeroes_most.append(bl_no_zeroes_most)\n",
    "    base_impact_without_zeroes_best.append(bl_no_zeroes_best)\n",
    "\n",
    "    confusion = confusion_matrix(y_test,y_pred)\n",
    "    matrix.append(confusion)\n",
    "    np.set_printoptions(suppress=True)    \n",
    "    reports.append(report)\n",
    "    \n",
    "    print('end: ', i)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+---------------------+----------------------+\n",
      "|     Algorithm     |      Precision      |        Recall       |          F1          |\n",
      "+-------------------+---------------------+---------------------+----------------------+\n",
      "|      als_ndcg     |  0.3071155684772349 | 0.44648323601645323 | 0.36381608896461026  |\n",
      "|      bpr_ndcg     |  0.2894430591660192 |  0.2650870319992754 |  0.2766261953834911  |\n",
      "|      lmf_ndcg     | 0.21879539122647942 |  0.0281647917045234 | 0.049725451459277405 |\n",
      "| most_popular_ndcg |  0.312153383202081  | 0.31874315744002046 | 0.31533179507951326  |\n",
      "|       zeroes      | 0.35583210523459485 |  0.4021451398370034 | 0.37735901018995965  |\n",
      "|        ---        |         ---         |         ---         |         ---          |\n",
      "|     macro avg     | 0.29666790146128186 |  0.2921246713994552 | 0.27657170821537036  |\n",
      "|    weighted avg   |  0.2996070465784301 |  0.3122452157297678 |  0.2911505427630906  |\n",
      "+-------------------+---------------------+---------------------+----------------------+\n",
      "Accuracy:  0.3122452157297678\n"
     ]
    }
   ],
   "source": [
    "avg_reports = report_average(reports)\n",
    "print_report(avg_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def print_report(avg_reports):\n",
    "    from prettytable import PrettyTable\n",
    "    x = PrettyTable()\n",
    "\n",
    "    x.field_names = [\"Algorithm\", \"Precision\", \"Recall\", \"F1\"]\n",
    "\n",
    "    for label in avg_reports.keys():\n",
    "        if label in 'accuracy':\n",
    "            x.add_row(['---','---','---','---'])\n",
    "            continue\n",
    "        x.add_row([label, \n",
    "                   avg_reports[label]['precision'], \n",
    "                   avg_reports[label]['recall'], \n",
    "                   avg_reports[label]['f1-score']])\n",
    "\n",
    "\n",
    "    print(x)\n",
    "    print('Accuracy: ', avg_reports['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def report_average(reports):\n",
    "    mean_dict = dict()\n",
    "    for label in reports[0].keys():\n",
    "        dictionary = dict()\n",
    "\n",
    "        if label in 'accuracy':\n",
    "            mean_dict[label] = sum(d[label] for d in reports) / len(reports)\n",
    "            continue\n",
    "\n",
    "        for key in reports[0][label].keys():\n",
    "            dictionary[key] = sum(d[label][key] for d in reports) / len(reports)\n",
    "        mean_dict[label] = dictionary\n",
    "\n",
    "    return mean_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grid_params = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed: 19.7min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 60.2min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 134.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
       "                                     batch_size='auto', beta_1=0.9,\n",
       "                                     beta_2=0.999, early_stopping=False,\n",
       "                                     epsilon=1e-08, hidden_layer_sizes=(100,),\n",
       "                                     learning_rate='constant',\n",
       "                                     learning_rate_init=0.001, max_iter=300,\n",
       "                                     momentum=0.9, n_iter_no_change=10,\n",
       "                                     nesterovs_momentum=True, power_t=0.5,\n",
       "                                     random_sta...\n",
       "                                     solver='adam', tol=0.0001,\n",
       "                                     validation_fraction=0.1, verbose=False,\n",
       "                                     warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'activation': ['tanh', 'relu'],\n",
       "                         'alpha': [0.0001, 0.05],\n",
       "                         'hidden_layer_sizes': [(50, 50, 50), (50, 100, 50),\n",
       "                                                (100,)],\n",
       "                         'learning_rate': ['constant', 'adaptive'],\n",
       "                         'solver': ['sgd', 'adam']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(mlp, grid_params, n_jobs=-1, cv=5, verbose=5)\n",
    "clf.fit(inputs,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'tanh',\n",
       " 'alpha': 0.05,\n",
       " 'hidden_layer_sizes': (100,),\n",
       " 'learning_rate': 'adaptive',\n",
       " 'solver': 'sgd'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
